{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "204c56f1-1c79-4e0d-b7c9-02fceb3e2844",
   "metadata": {},
   "source": [
    "# üì° Uso de APIs LLM - OpenAI e Anthropic\n",
    " \n",
    "## Este notebook ensina como usar as principais APIs de LLM para an√°lise de c√≥digo.\n",
    " \n",
    "### ## Objetivos:\n",
    "### 1. Configurar APIs OpenAI e Anthropic\n",
    "### 2. Comparar diferentes modelos\n",
    "### 3. Entender par√¢metros importantes\n",
    "### 4. Criar um comparador para an√°lise de c√≥digo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22211989-7340-4ca0-83b4-67d406e28ade",
   "metadata": {},
   "source": [
    "### 1. Instala√ß√£o e Configura√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df58d39f-6d41-4d78-9d2c-72914b2c39af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar bibliotecas necess√°rias\n",
    "#!pip install openai anthropic python-dotenv tiktoken\n",
    "#!pip uninstall anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63b6b235-43cf-4c68-8b23-06b0f954f987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\pcwin\\anaconda3\\envs\\esp_ai\\lib\\site-packages (2.14.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\pcwin\\anaconda3\\envs\\esp_ai\\lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\pcwin\\anaconda3\\envs\\esp_ai\\lib\\site-packages (0.12.0)\n",
      "Collecting ollama\n",
      "  Downloading ollama-0.6.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\pcwin\\anaconda3\\envs\\esp_ai\\lib\\site-packages (from openai) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\pcwin\\anaconda3\\envs\\esp_ai\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\pcwin\\anaconda3\\envs\\esp_ai\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in c:\\users\\pcwin\\anaconda3\\envs\\esp_ai\\lib\\site-packages (from openai) (0.12.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\pcwin\\anaconda3\\envs\\esp_ai\\lib\\site-packages (from openai) (2.12.5)\n",
      "Requirement already satisfied: sniffio in c:\\users\\pcwin\\anaconda3\\envs\\esp_ai\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\pcwin\\anaconda3\\envs\\esp_ai\\lib\\site-packages (from openai) (4.66.6)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\pcwin\\anaconda3\\envs\\esp_ai\\lib\\site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\pcwin\\anaconda3\\envs\\esp_ai\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
      "Requirement already satisfied: certifi in c:\\users\\pcwin\\anaconda3\\envs\\esp_ai\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\pcwin\\anaconda3\\envs\\esp_ai\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\pcwin\\anaconda3\\envs\\esp_ai\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\pcwin\\anaconda3\\envs\\esp_ai\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\pcwin\\anaconda3\\envs\\esp_ai\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\pcwin\\anaconda3\\envs\\esp_ai\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\pcwin\\anaconda3\\envs\\esp_ai\\lib\\site-packages (from tiktoken) (2025.11.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\pcwin\\anaconda3\\envs\\esp_ai\\lib\\site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pcwin\\anaconda3\\envs\\esp_ai\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pcwin\\anaconda3\\envs\\esp_ai\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\pcwin\\anaconda3\\envs\\esp_ai\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Downloading ollama-0.6.1-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: ollama\n",
      "Successfully installed ollama-0.6.1\n"
     ]
    }
   ],
   "source": [
    "# Instalar bibliotecas necess√°rias\n",
    "!pip install openai python-dotenv tiktoken ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98af730-0cbc-429e-be07-c6f9f8ac7c47",
   "metadata": {},
   "source": [
    "### üîë 2. Configura√ß√£o via Arquivo .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f7bcda9-ff3b-409a-963f-ecd4e8f654de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üîç CONFIGURA√á√ÉO ATUAL:\n",
      "==================================================\n",
      "‚úÖ Arquivo .env: .env\n",
      "üîë OpenAI API Key: ‚úÖ Configurada\n",
      "ü§ñ Ollama Model: llama3\n",
      "üåê Ollama URL: http://localhost:11434\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Carregar vari√°veis do arquivo .env\n",
    "load_dotenv()\n",
    "\n",
    "# Verificar e configurar .env se necess√°rio\n",
    "def setup_environment():\n",
    "    \"\"\"Configura o ambiente e cria arquivo .env se n√£o existir\"\"\"\n",
    "    \n",
    "    env_file = '.env'\n",
    "    \n",
    "    if not os.path.exists(env_file):\n",
    "        print(\"üìù Criando arquivo .env...\")\n",
    "        \n",
    "        # Pedir OpenAI API Key ao usu√°rio\n",
    "        openai_key = input(\"Digite sua OpenAI API Key (ou Enter para pular): \").strip()\n",
    "        \n",
    "        # Criar conte√∫do do .env\n",
    "        env_content = f\"\"\"# OpenAI API Key (obtenha em: https://platform.openai.com/api-keys)\n",
    "# Come√ßa com 'sk-'\n",
    "OPENAI_API_KEY={openai_key if openai_key else 'sua-chave-openai-aqui'}\n",
    "\n",
    "# Configura√ß√µes do Ollama (local)\n",
    "OLLAMA_MODEL=llama3\n",
    "OLLAMA_BASE_URL=http://localhost:11434\n",
    "\n",
    "# Configura√ß√µes da aplica√ß√£o\n",
    "TEMPERATURE_CODE_ANALYSIS=0.1\n",
    "MAX_TOKENS_CODE_ANALYSIS=500\n",
    "\"\"\"\n",
    "        \n",
    "        with open(env_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(env_content)\n",
    "        \n",
    "        print(f\"‚úÖ Arquivo {env_file} criado com sucesso!\")\n",
    "        print(\"üìã Voc√™ pode edit√°-lo manualmente para atualizar as configura√ß√µes.\")\n",
    "    \n",
    "    # Recarregar vari√°veis\n",
    "    load_dotenv(override=True)\n",
    "    \n",
    "    return env_file\n",
    "\n",
    "# Executar setup\n",
    "env_file = setup_environment()\n",
    "\n",
    "# Verificar configura√ß√µes\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OLLAMA_MODEL = os.getenv(\"OLLAMA_MODEL\", \"llama3\")\n",
    "OLLAMA_BASE_URL = os.getenv(\"OLLAMA_BASE_URL\", \"http://localhost:11434\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üîç CONFIGURA√á√ÉO ATUAL:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"‚úÖ Arquivo .env: {env_file}\")\n",
    "print(f\"üîë OpenAI API Key: {'‚úÖ Configurada' if OPENAI_API_KEY and OPENAI_API_KEY != 'sua-chave-openai-aqui' else '‚ùå N√£o configurada'}\")\n",
    "print(f\"ü§ñ Ollama Model: {OLLAMA_MODEL}\")\n",
    "print(f\"üåê Ollama URL: {OLLAMA_BASE_URL}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cff4c8-f454-4cae-9e79-e32f6b7f8ac6",
   "metadata": {},
   "source": [
    "### ü§ñ 3. Configurar Ollama (Local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "149c478d-67e0-4736-bb39-2a9b09185524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üîÑ CONFIGURANDO OLLAMA (LOCAL)\n",
      "==================================================\n",
      "‚ùå Ollama n√£o est√° instalado.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Deseja instalar o Ollama agora? (s/n):  s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  Para Windows, baixe manualmente em: https://ollama.com/download\n"
     ]
    }
   ],
   "source": [
    "def setup_ollama():\n",
    "    \"\"\"Configura e verifica o Ollama local\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üîÑ CONFIGURANDO OLLAMA (LOCAL)\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    try:\n",
    "        # Verificar se Ollama est√° instalado\n",
    "        result = subprocess.run(['ollama', '--version'], \n",
    "                              capture_output=True, text=True)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(f\"‚úÖ Ollama encontrado: {result.stdout.strip()}\")\n",
    "        else:\n",
    "            print(\"‚ùå Ollama n√£o encontrado. Tentando instalar...\")\n",
    "            \n",
    "            # Instalar Ollama (Linux/Mac)\n",
    "            if sys.platform != 'win32':\n",
    "                !curl -fsSL https://ollama.com/install.sh | sh\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è  Para Windows, baixe em: https://ollama.com/download\")\n",
    "                print(\"‚ö†Ô∏è  Execute o instalador e reinicie o terminal.\")\n",
    "                return False\n",
    "            \n",
    "            # Verificar novamente\n",
    "            result = subprocess.run(['ollama', '--version'], \n",
    "                                  capture_output=True, text=True)\n",
    "            if result.returncode != 0:\n",
    "                print(\"‚ùå Falha na instala√ß√£o do Ollama\")\n",
    "                return False\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå Ollama n√£o est√° instalado.\")\n",
    "        install = input(\"Deseja instalar o Ollama agora? (s/n): \")\n",
    "        \n",
    "        if install.lower() == 's':\n",
    "            if sys.platform != 'win32':\n",
    "                !curl -fsSL https://ollama.com/install.sh | sh\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è  Para Windows, baixe manualmente em: https://ollama.com/download\")\n",
    "                return False\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  Ollama n√£o ser√° usado. Continuando apenas com OpenAI.\")\n",
    "            return False\n",
    "    \n",
    "    # Verificar/puxar modelo\n",
    "    try:\n",
    "        print(f\"\\nüîç Verificando modelo '{OLLAMA_MODEL}'...\")\n",
    "        result = subprocess.run(['ollama', 'list'], \n",
    "                              capture_output=True, text=True)\n",
    "        \n",
    "        if OLLAMA_MODEL in result.stdout:\n",
    "            print(f\"‚úÖ Modelo '{OLLAMA_MODEL}' j√° est√° baixado.\")\n",
    "        else:\n",
    "            print(f\"‚¨áÔ∏è  Baixando modelo '{OLLAMA_MODEL}'...\")\n",
    "            !ollama pull {OLLAMA_MODEL}\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao configurar Ollama: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Executar setup do Ollama\n",
    "ollama_ready = setup_ollama()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2793eab-babf-49a4-a63e-f0cb6266c65f",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è 4. Inicializar Clientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf56e9a-43ee-4c28-8849-580115692db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import ollama\n",
    "\n",
    "# Configurar cliente OpenAI\n",
    "openai_client = None\n",
    "if OPENAI_API_KEY and OPENAI_API_KEY != 'sua-chave-openai-aqui':\n",
    "    try:\n",
    "        openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "        print(\"‚úÖ Cliente OpenAI configurado com sucesso!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao configurar OpenAI: {str(e)}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  OpenAI n√£o configurado. Usando apenas Ollama local.\")\n",
    "\n",
    "# Configurar Ollama\n",
    "if ollama_ready:\n",
    "    try:\n",
    "        # Testar conex√£o com Ollama\n",
    "        response = ollama.chat(model=OLLAMA_MODEL, messages=[\n",
    "            {'role': 'user', 'content': 'Hello'}\n",
    "        ])\n",
    "        print(f\"‚úÖ Ollama conectado! Modelo: {OLLAMA_MODEL}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao conectar com Ollama: {str(e)}\")\n",
    "        ollama_ready = False\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Ollama n√£o dispon√≠vel.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìä STATUS DOS CLIENTES:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"OpenAI: {'‚úÖ Pronto' if openai_client else '‚ùå N√£o dispon√≠vel'}\")\n",
    "print(f\"Ollama: {'‚úÖ Pronto' if ollama_ready else '‚ùå N√£o dispon√≠vel'}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if not openai_client and not ollama_ready:\n",
    "    print(\"\\n‚ö†Ô∏è  ATEN√á√ÉO: Nenhum cliente LLM configurado!\")\n",
    "    print(\"Configure pelo menos um dos dois para continuar.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfdeef4-33d2-4924-81d8-72844a12dca5",
   "metadata": {},
   "source": [
    "### üß™ 5. Fun√ß√µes B√°sicas de An√°lise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef8ef58-15a7-4f02-8c9e-126319b22953",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_with_openai(code, question, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"Analisa c√≥digo usando OpenAI\"\"\"\n",
    "    \n",
    "    if not openai_client:\n",
    "        return \"‚ùå OpenAI n√£o configurado\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Voc√™ √© um especialista em an√°lise de c√≥digo Python.\n",
    "    \n",
    "    Por favor, analise este c√≥digo:\n",
    "    \n",
    "    ```python\n",
    "    {code}\n",
    "    ```\n",
    "    \n",
    "    Responda √† pergunta: {question}\n",
    "    \n",
    "    Forne√ßa uma an√°lise t√©cnica detalhada.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Voc√™ √© um analista de c√≥digo especializado.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=int(os.getenv(\"MAX_TOKENS_CODE_ANALYSIS\", 500)),\n",
    "            temperature=float(os.getenv(\"TEMPERATURE_CODE_ANALYSIS\", 0.1))\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Erro OpenAI: {str(e)}\"\n",
    "\n",
    "def analyze_with_ollama(code, question):\n",
    "    \"\"\"Analisa c√≥digo usando Ollama local\"\"\"\n",
    "    \n",
    "    if not ollama_ready:\n",
    "        return \"‚ùå Ollama n√£o configurado\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Voc√™ √© um especialista em an√°lise de c√≥digo Python.\n",
    "    \n",
    "    Por favor, analise este c√≥digo:\n",
    "    \n",
    "    ```python\n",
    "    {code}\n",
    "    ```\n",
    "    \n",
    "    Responda √† pergunta: {question}\n",
    "    \n",
    "    Forne√ßa uma an√°lise t√©cnica detalhada em portugu√™s.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=OLLAMA_MODEL,\n",
    "            messages=[\n",
    "                {'role': 'system', 'content': 'Voc√™ √© um analista de c√≥digo especializado.'},\n",
    "                {'role': 'user', 'content': prompt}\n",
    "            ],\n",
    "            options={\n",
    "                'temperature': float(os.getenv(\"TEMPERATURE_CODE_ANALYSIS\", 0.1)),\n",
    "                'num_predict': int(os.getenv(\"MAX_TOKENS_CODE_ANALYSIS\", 500))\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return response['message']['content']\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Erro Ollama: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd350452-2f90-4a1b-af9d-3dac34204548",
   "metadata": {},
   "source": [
    "### üîç 6. Teste Pr√°tico: Analisar C√≥digo Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87cc9d9-75cb-4286-bd1e-c771ca7ec304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo exemplo do seu reposit√≥rio ESPECIALISTA_EM_AI\n",
    "sample_codes = {\n",
    "    \"CNN simples\": \"\"\"\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def create_simple_cnn(input_shape=(32, 32, 3), num_classes=10):\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\"\"\",\n",
    "    \n",
    "    \"Busca bin√°ria\": \"\"\"\n",
    "def binary_search(arr, target):\n",
    "    left, right = 0, len(arr) - 1\n",
    "    \n",
    "    while left <= right:\n",
    "        mid = (left + right) // 2\n",
    "        \n",
    "        if arr[mid] == target:\n",
    "            return mid\n",
    "        elif arr[mid] < target:\n",
    "            left = mid + 1\n",
    "        else:\n",
    "            right = mid - 1\n",
    "    \n",
    "    return -1\n",
    "\"\"\",\n",
    "    \n",
    "    \"Processamento de texto\": \"\"\"\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def analyze_text(text):\n",
    "    # Limpar texto\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\\\s]', '', text)\n",
    "    \n",
    "    # Contar palavras\n",
    "    words = text.split()\n",
    "    word_count = len(words)\n",
    "    unique_words = len(set(words))\n",
    "    \n",
    "    # Palavras mais comuns\n",
    "    common_words = Counter(words).most_common(5)\n",
    "    \n",
    "    return {\n",
    "        'total_words': word_count,\n",
    "        'unique_words': unique_words,\n",
    "        'common_words': common_words\n",
    "    }\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "# Perguntas para teste\n",
    "test_questions = [\n",
    "    \"Esta √© uma boa implementa√ß√£o? Quais melhorias voc√™ sugere?\",\n",
    "    \"Qual √© a complexidade deste algoritmo?\",\n",
    "    \"Existe algum bug potencial neste c√≥digo?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0efc467-3289-471f-af09-e28079d3a0b9",
   "metadata": {},
   "source": [
    "### üìä 7. Executar An√°lises Comparativas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48c2090-87c3-4887-ae40-cc5a19630349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def run_comparative_analysis():\n",
    "    \"\"\"Executa an√°lise comparativa entre OpenAI e Ollama\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for code_name, code in sample_codes.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üîç ANALISANDO: {code_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        for question in test_questions:\n",
    "            print(f\"\\n‚ùì Pergunta: {question}\")\n",
    "            print(f\"{'-'*40}\")\n",
    "            \n",
    "            # Analisar com OpenAI (se dispon√≠vel)\n",
    "            if openai_client:\n",
    "                print(\"ü§ñ OpenAI GPT-3.5-Turbo:\")\n",
    "                start_time = time.time()\n",
    "                openai_result = analyze_with_openai(code, question)\n",
    "                openai_time = time.time() - start_time\n",
    "                \n",
    "                # Mostrar resumo\n",
    "                print(f\"   ‚è±Ô∏è  Tempo: {openai_time:.2f}s\")\n",
    "                print(f\"   üìù Resumo: {openai_result[:100]}...\")\n",
    "                \n",
    "                results.append({\n",
    "                    'C√≥digo': code_name,\n",
    "                    'Pergunta': question[:30] + \"...\",\n",
    "                    'Modelo': 'OpenAI GPT-3.5',\n",
    "                    'Tempo (s)': round(openai_time, 2),\n",
    "                    'Comprimento': len(openai_result),\n",
    "                    'Resposta': openai_result[:150] + \"...\"\n",
    "                })\n",
    "            \n",
    "            # Analisar com Ollama (se dispon√≠vel)\n",
    "            if ollama_ready:\n",
    "                print(\"\\nüñ•Ô∏è  Ollama (Local):\")\n",
    "                start_time = time.time()\n",
    "                ollama_result = analyze_with_ollama(code, question)\n",
    "                ollama_time = time.time() - start_time\n",
    "                \n",
    "                # Mostrar resumo\n",
    "                print(f\"   ‚è±Ô∏è  Tempo: {ollama_time:.2f}s\")\n",
    "                print(f\"   üìù Resumo: {ollama_result[:100]}...\")\n",
    "                \n",
    "                results.append({\n",
    "                    'C√≥digo': code_name,\n",
    "                    'Pergunta': question[:30] + \"...\",\n",
    "                    'Modelo': f'Ollama {OLLAMA_MODEL}',\n",
    "                    'Tempo (s)': round(ollama_time, 2),\n",
    "                    'Comprimento': len(ollama_result),\n",
    "                    'Resposta': ollama_result[:150] + \"...\"\n",
    "                })\n",
    "            \n",
    "            print(f\"{'-'*40}\")\n",
    "    \n",
    "    # Criar DataFrame com resultados\n",
    "    if results:\n",
    "        df = pd.DataFrame(results)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"üìä RESUMO COMPARATIVO\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Agrupar por modelo para estat√≠sticas\n",
    "        if len(df) > 0:\n",
    "            stats = df.groupby('Modelo').agg({\n",
    "                'Tempo (s)': ['mean', 'min', 'max'],\n",
    "                'Comprimento': 'mean'\n",
    "            }).round(2)\n",
    "            \n",
    "            display(stats)\n",
    "        \n",
    "        return df\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Nenhum resultado para mostrar.\")\n",
    "        return None\n",
    "\n",
    "# Executar an√°lises\n",
    "comparison_df = run_comparative_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574de379-ad2c-4f25-8926-63c447089e65",
   "metadata": {},
   "source": [
    "### ‚ö° 8. Streaming em Tempo Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b22d44b-cad2-4e3f-aed9-c7c28aad5e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_analysis(code, question, provider=\"ollama\"):\n",
    "    \"\"\"Mostra an√°lise com streaming em tempo real\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Analise este c√≥digo Python:\n",
    "    \n",
    "    ```python\n",
    "    {code}\n",
    "    ```\n",
    "    \n",
    "    Pergunta: {question}\n",
    "    \n",
    "    Responda de forma clara e t√©cnica.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nüéØ Analisando com {provider.upper()}...\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    if provider == \"openai\" and openai_client:\n",
    "        try:\n",
    "            stream = openai_client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                max_tokens=300,\n",
    "                temperature=0.1,\n",
    "                stream=True\n",
    "            )\n",
    "            \n",
    "            for chunk in stream:\n",
    "                if chunk.choices[0].delta.content is not None:\n",
    "                    print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Erro: {str(e)}\")\n",
    "    \n",
    "    elif provider == \"ollama\" and ollama_ready:\n",
    "        try:\n",
    "            # Ollama tamb√©m suporta streaming\n",
    "            response = ollama.generate(\n",
    "                model=OLLAMA_MODEL,\n",
    "                prompt=prompt,\n",
    "                stream=True\n",
    "            )\n",
    "            \n",
    "            for chunk in response:\n",
    "                if 'response' in chunk:\n",
    "                    print(chunk['response'], end=\"\", flush=True)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Erro: {str(e)}\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"‚ùå Provedor '{provider}' n√£o dispon√≠vel.\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Testar streaming (escolha um provedor)\n",
    "if openai_client or ollama_ready:\n",
    "    # Usar primeiro provedor dispon√≠vel\n",
    "    provider = \"openai\" if openai_client else \"ollama\"\n",
    "    stream_analysis(sample_codes[\"Busca bin√°ria\"], \n",
    "                   \"Esta implementa√ß√£o est√° correta?\", \n",
    "                   provider=provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e13b1df-7847-45fa-bf26-6a130700c3a0",
   "metadata": {},
   "source": [
    "### üìà 9. Benchmark de Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b292cad4-f5be-4ede-93c5-0e1e289cd8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmark(iterations=2):\n",
    "    \"\"\"Executa benchmark de performance\"\"\"\n",
    "    \n",
    "    benchmark_results = []\n",
    "    code_to_test = sample_codes[\"Busca bin√°ria\"]\n",
    "    question = \"Qual √© a complexidade deste algoritmo?\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üèÉ‚Äç‚ôÇÔ∏è  BENCHMARK ({iterations} itera√ß√µes)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Testar OpenAI\n",
    "    if openai_client:\n",
    "        print(\"\\nü§ñ Testando OpenAI GPT-3.5-Turbo...\")\n",
    "        openai_times = []\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            start_time = time.time()\n",
    "            result = analyze_with_openai(code_to_test, question)\n",
    "            elapsed = time.time() - start_time\n",
    "            openai_times.append(elapsed)\n",
    "            \n",
    "            print(f\"  Itera√ß√£o {i+1}: {elapsed:.2f}s ({len(result)} chars)\")\n",
    "        \n",
    "        avg_time = sum(openai_times) / len(openai_times)\n",
    "        benchmark_results.append({\n",
    "            'Modelo': 'OpenAI GPT-3.5',\n",
    "            'Tempo M√©dio (s)': round(avg_time, 2),\n",
    "            'Itera√ß√µes': iterations,\n",
    "            'Tipo': 'Cloud'\n",
    "        })\n",
    "    \n",
    "    # Testar Ollama\n",
    "    if ollama_ready:\n",
    "        print(f\"\\nüñ•Ô∏è  Testando Ollama {OLLAMA_MODEL}...\")\n",
    "        ollama_times = []\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            start_time = time.time()\n",
    "            result = analyze_with_ollama(code_to_test, question)\n",
    "            elapsed = time.time() - start_time\n",
    "            ollama_times.append(elapsed)\n",
    "            \n",
    "            print(f\"  Itera√ß√£o {i+1}: {elapsed:.2f}s ({len(result)} chars)\")\n",
    "        \n",
    "        avg_time = sum(ollama_times) / len(ollama_times)\n",
    "        benchmark_results.append({\n",
    "            'Modelo': f'Ollama {OLLAMA_MODEL}',\n",
    "            'Tempo M√©dio (s)': round(avg_time, 2),\n",
    "            'Itera√ß√µes': iterations,\n",
    "            'Tipo': 'Local'\n",
    "        })\n",
    "    \n",
    "    # Mostrar resultados\n",
    "    if benchmark_results:\n",
    "        benchmark_df = pd.DataFrame(benchmark_results)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"üìä RESULTADOS DO BENCHMARK\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        display(benchmark_df)\n",
    "        \n",
    "        # An√°lise\n",
    "        print(\"\\nüí° AN√ÅLISE:\")\n",
    "        for _, row in benchmark_df.iterrows():\n",
    "            print(f\"  ‚Ä¢ {row['Modelo']}: {row['Tempo M√©dio (s)']}s ({row['Tipo']})\")\n",
    "        \n",
    "        return benchmark_df\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Nenhum modelo dispon√≠vel para benchmark.\")\n",
    "        return None\n",
    "\n",
    "# Executar benchmark\n",
    "benchmark_df = run_benchmark(iterations=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6b6d2e-4371-4d88-981f-4c2e6a684bd7",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è 10. Sistema de An√°lise Inteligente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296efefc-a657-45dd-b65a-a32b51f2c40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeAnalysisAssistant:\n",
    "    \"\"\"Assistente inteligente para an√°lise de c√≥digo\"\"\"\n",
    "    \n",
    "    def __init__(self, use_openai=True, use_ollama=True):\n",
    "        self.use_openai = use_openai and openai_client is not None\n",
    "        self.use_ollama = use_ollama and ollama_ready\n",
    "        \n",
    "        print(f\"ü§ñ Assistente inicializado:\")\n",
    "        print(f\"   ‚Ä¢ OpenAI: {'‚úÖ' if self.use_openai else '‚ùå'}\")\n",
    "        print(f\"   ‚Ä¢ Ollama: {'‚úÖ' if self.use_ollama else '‚ùå'}\")\n",
    "    \n",
    "    def smart_analyze(self, code, question):\n",
    "        \"\"\"Escolhe o melhor modelo para an√°lise\"\"\"\n",
    "        \n",
    "        # Heur√≠stica simples: usa Ollama para c√≥digos curtos, OpenAI para complexos\n",
    "        code_complexity = self._estimate_complexity(code)\n",
    "        \n",
    "        if code_complexity == \"alta\" and self.use_openai:\n",
    "            print(\"üîç Usando OpenAI (c√≥digo complexo)...\")\n",
    "            return analyze_with_openai(code, question)\n",
    "        elif self.use_ollama:\n",
    "            print(\"üîç Usando Ollama (c√≥digo simples/local)...\")\n",
    "            return analyze_with_ollama(code, question)\n",
    "        elif self.use_openai:\n",
    "            print(\"üîç Usando OpenAI (fallback)...\")\n",
    "            return analyze_with_openai(code, question)\n",
    "        else:\n",
    "            return \"‚ùå Nenhum modelo dispon√≠vel para an√°lise.\"\n",
    "    \n",
    "    def _estimate_complexity(self, code):\n",
    "        \"\"\"Estima complexidade do c√≥digo\"\"\"\n",
    "        lines = code.strip().split('\\n')\n",
    "        \n",
    "        if len(lines) < 10:\n",
    "            return \"baixa\"\n",
    "        elif len(lines) < 30:\n",
    "            return \"m√©dia\"\n",
    "        else:\n",
    "            return \"alta\"\n",
    "    \n",
    "    def compare_analyses(self, code, question):\n",
    "        \"\"\"Compara an√°lises de diferentes modelos\"\"\"\n",
    "        \n",
    "        analyses = {}\n",
    "        \n",
    "        if self.use_openai:\n",
    "            print(\"\\nü§ñ Obtendo an√°lise da OpenAI...\")\n",
    "            analyses['OpenAI'] = analyze_with_openai(code, question)\n",
    "        \n",
    "        if self.use_ollama:\n",
    "            print(\"üñ•Ô∏è  Obtendo an√°lise do Ollama...\")\n",
    "            analyses['Ollama'] = analyze_with_ollama(code, question)\n",
    "        \n",
    "        # Mostrar compara√ß√£o\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"üÜö COMPARA√á√ÉO DE AN√ÅLISES\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        for model, analysis in analyses.items():\n",
    "            print(f\"\\nüìã {model}:\")\n",
    "            print(f\"{'‚îÄ'*40}\")\n",
    "            print(analysis[:300] + \"...\" if len(analysis) > 300 else analysis)\n",
    "        \n",
    "        return analyses\n",
    "\n",
    "# Testar o assistente\n",
    "if openai_client or ollama_ready:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üß™ TESTANDO ASSISTENTE INTELIGENTE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    assistant = CodeAnalysisAssistant()\n",
    "    \n",
    "    # Testar an√°lise inteligente\n",
    "    test_code = sample_codes[\"Processamento de texto\"]\n",
    "    test_question = \"Esta fun√ß√£o √© eficiente para processar grandes textos?\"\n",
    "    \n",
    "    print(f\"\\n‚ùì Pergunta: {test_question}\")\n",
    "    result = assistant.smart_analyze(test_code, test_question)\n",
    "    print(f\"\\nüìù Resposta: {result[:200]}...\")\n",
    "    \n",
    "    # Testar compara√ß√£o\n",
    "    if assistant.use_openai and assistant.use_ollama:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"üîÑ COMPARA√á√ÉO DETALHADA\")\n",
    "        print(\"=\"*60)\n",
    "        assistant.compare_analyses(test_code, test_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17f3943-e69d-4eb7-85f7-e085a039feb6",
   "metadata": {},
   "source": [
    "### üíæ 11. Salvar Resultados e Configura√ß√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0767af6a-1c2b-413a-a065-97cea5e2ecd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def save_analysis_session(results, filename=\"analysis_session.json\"):\n",
    "    \"\"\"Salva toda a sess√£o de an√°lise para refer√™ncia futura\"\"\"\n",
    "    \n",
    "    session_data = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"config\": {\n",
    "            \"openai_available\": openai_client is not None,\n",
    "            \"ollama_available\": ollama_ready,\n",
    "            \"ollama_model\": OLLAMA_MODEL,\n",
    "            \"temperature\": os.getenv(\"TEMPERATURE_CODE_ANALYSIS\"),\n",
    "            \"max_tokens\": os.getenv(\"MAX_TOKENS_CODE_ANALYSIS\")\n",
    "        },\n",
    "        \"sample_codes\": {k: v[:100] + \"...\" for k, v in sample_codes.items()},\n",
    "        \"results\": results\n",
    "    }\n",
    "    \n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(session_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ Sess√£o salva em: {filename}\")\n",
    "    \n",
    "    # Tamb√©m salvar um resumo em Markdown\n",
    "    summary_file = \"analysis_summary.md\"\n",
    "    with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"# Resumo da An√°lise de C√≥digo\\n\\n\")\n",
    "        f.write(f\"**Data**: {datetime.now().strftime('%d/%m/%Y %H:%M')}\\n\\n\")\n",
    "        \n",
    "        f.write(\"## Configura√ß√£o\\n\")\n",
    "        f.write(f\"- OpenAI: {'‚úÖ' if openai_client else '‚ùå'}\\n\")\n",
    "        f.write(f\"- Ollama: {'‚úÖ' if ollama_ready else '‚ùå'} ({OLLAMA_MODEL})\\n\\n\")\n",
    "        \n",
    "        if comparison_df is not None:\n",
    "            f.write(\"## Resultados Comparativos\\n\")\n",
    "            f.write(comparison_df.to_markdown(index=False))\n",
    "            f.write(\"\\n\\n\")\n",
    "        \n",
    "        if benchmark_df is not None:\n",
    "            f.write(\"## Benchmark de Performance\\n\")\n",
    "            f.write(benchmark_df.to_markdown(index=False))\n",
    "    \n",
    "    print(f\"üìù Resumo em Markdown: {summary_file}\")\n",
    "    \n",
    "    return session_data\n",
    "\n",
    "# Salvar resultados se houver\n",
    "if comparison_df is not None:\n",
    "    save_analysis_session(comparison_df.to_dict('records'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fde8caa-8c0a-4fc0-ba9f-659cbe108147",
   "metadata": {},
   "source": [
    "### üéØ 12. Exerc√≠cio: Analise Seu Pr√≥prio C√≥digo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a10756-1289-44d0-bb48-e2eccbb7c76b",
   "metadata": {},
   "source": [
    "### C√âLULA PARA VOC√ä TESTAR COM UM PR√ìPRIO C√ìDIGO\n",
    "\n",
    "### Instru√ß√µes:\n",
    "### 1. Cole abaixo um trecho do SEU c√≥digo do ESPECIALISTA_EM_AI\n",
    "### 2. Fa√ßa uma pergunta espec√≠fica sobre ele\n",
    "### 3. Execute as c√©lulas para ver a an√°lise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8739123-32dd-4d1d-b54f-0392596449b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COLE SEU C√ìDIGO AQUI\n",
    "meu_codigo = \"\"\"\n",
    "# Cole aqui um trecho do seu reposit√≥rio\n",
    "# Exemplo: fun√ß√£o do projeto de reconhecimento facial\n",
    "\n",
    "def minha_funcao():\n",
    "    # implementa√ß√£o...\n",
    "    pass\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63163da4-f436-47ec-8fc0-f2927f838ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FA√áA SUA PERGUNTA AQUI\n",
    "minha_pergunta = \"Esta implementa√ß√£o est√° correta? Como poderia melhorar?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d75ab31-869d-44b3-a60f-8c216fcd6fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXECUTAR AN√ÅLISE\n",
    "if meu_codigo.strip() and meu_codigo != '# Cole aqui um trecho do seu reposit√≥rio\\n# Exemplo: fun√ß√£o do projeto de reconhecimento facial\\n\\ndef minha_funcao():\\n    # implementa√ß√£o...\\n    pass':\n",
    "    print(\"üîç Analisando seu c√≥digo...\\n\")\n",
    "    \n",
    "    # Usar assistente inteligente\n",
    "    if openai_client or ollama_ready:\n",
    "        assistant = CodeAnalysisAssistant()\n",
    "        \n",
    "        print(f\"üìã Seu c√≥digo ({len(meu_codigo.split())} palavras):\")\n",
    "        print(\"=\"*50)\n",
    "        print(meu_codigo[:500] + \"...\" if len(meu_codigo) > 500 else meu_codigo)\n",
    "        print(\"=\"*50)\n",
    "        print(f\"\\n‚ùì Sua pergunta: {minha_pergunta}\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # An√°lise inteligente\n",
    "        resultado = assistant.smart_analyze(meu_codigo, minha_pergunta)\n",
    "        \n",
    "        print(\"\\nüí° AN√ÅLISE:\")\n",
    "        print(\"=\"*50)\n",
    "        print(resultado)\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Oferecer compara√ß√£o se ambos estiverem dispon√≠veis\n",
    "        if assistant.use_openai and assistant.use_ollama:\n",
    "            resposta = input(\"\\nüìä Deseja comparar an√°lises OpenAI vs Ollama? (s/n): \")\n",
    "            if resposta.lower() == 's':\n",
    "                assistant.compare_analyses(meu_codigo, minha_pergunta)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Configure pelo menos um modelo (OpenAI ou Ollama) primeiro.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Cole seu c√≥digo na c√©lula acima primeiro!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8954c6-64b3-4317-b9a6-777a6e5e19d7",
   "metadata": {},
   "source": [
    "### üìö 13. Conclus√£o e Pr√≥ximos Passos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856a9892-117c-448f-af18-f01bf590afab",
   "metadata": {},
   "source": [
    "### üéØ RESUMO DO NOTEBOOK\n",
    "\n",
    "### ‚úÖ O que aprendemos e configuramos:\n",
    "1. **Configura√ß√£o via .env**: M√©todo seguro para armazenar chaves\n",
    "2. **OpenAI API**: Configura√ß√£o e uso para an√°lise de c√≥digo\n",
    "3. **Ollama Local**: Instala√ß√£o e uso de modelos locais gratuitos\n",
    "4. **An√°lise Comparativa**: Cloud vs Local, performance, qualidade\n",
    "5. **Sistema Inteligente**: Escolha autom√°tica do melhor modelo\n",
    "\n",
    "### üîß Configura√ß√£o atual:\n",
    "- OpenAI: {'‚úÖ Pronto' if openai_client else '‚ùå N√£o configurado'}\n",
    "- Ollama: {'‚úÖ Pronto' if ollama_ready else '‚ùå N√£o configurado'} ({OLLAMA_MODEL})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c3a03a-d5f8-43f3-b789-fa9513c57e51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Especialista IA",
   "language": "python",
   "name": "esp_ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
