{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5b3583-d0ff-4dda-9dd9-4416a8e09ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ö° 03 - Infer√™ncia Eficiente para GPU 2GB\n",
    "## üéØ Maximizando Performance no Hardware Limitado\n",
    "\n",
    "**Problema anterior resolvido:** Alguns modelos BERT n√£o suportam `device_map='auto'`\n",
    "**Solu√ß√£o:** Carregamento manual otimizado + t√©cnicas de infer√™ncia eficiente\n",
    "\n",
    "---\n",
    "\n",
    "## üìä RESULTADOS DO NOTEBOOK 02\n",
    "‚úÖ GPT-2 PT: 0.8GB | Funciona perfeitamente  \n",
    "‚ö†Ô∏è BERT Legal: Precisa carregamento manual (sem device_map)  \n",
    "üéØ Mem√≥ria livre: 1.3GB (√≥timo para otimiza√ß√µes)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ OBJETIVOS DESTE NOTEBOOK\n",
    "1. Carregar BERT jur√≠dico SEM device_map='auto'\n",
    "2. Otimizar infer√™ncia com float16 e chunking\n",
    "3. Implementar cache de aten√ß√£o para documentos longos\n",
    "4. Testar quantiza√ß√£o 8-bit (se bitsandbytes instalado)\n",
    "5. Criar pipeline jur√≠dico ULTRA eficiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d262f1e3-6237-4dca-acd1-ae32811ac455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "‚ö° INFER√äNCIA EFICIENTE - CONFIGURA√á√ÉO\n",
      "======================================================================\n",
      "‚úÖ bitsandbytes instalado - Podemos usar quantiza√ß√£o 8-bit!\n",
      "\n",
      "üíª GPU: NVIDIA GeForce 930M\n",
      "üíæ VRAM Total: 2.1 GB\n",
      "üßπ Cache limpo - Mem√≥ria inicial: 0.00 GB\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Configura√ß√£o Inicial\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModelForSequenceClassification\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"‚ö° INFER√äNCIA EFICIENTE - CONFIGURA√á√ÉO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Verificar se bitsandbytes est√° instalado para quantiza√ß√£o\n",
    "try:\n",
    "    import bitsandbytes as bnb\n",
    "    BITSANDBYTES_AVAILABLE = True\n",
    "    print(\"‚úÖ bitsandbytes instalado - Podemos usar quantiza√ß√£o 8-bit!\")\n",
    "except ImportError:\n",
    "    BITSANDBYTES_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  bitsandbytes n√£o instalado. Para quantiza√ß√£o: pip install bitsandbytes\")\n",
    "\n",
    "# Status da GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nüíª GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üíæ VRAM Total: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    \n",
    "    # Limpar cache para come√ßar fresco\n",
    "    torch.cuda.empty_cache()\n",
    "    memoria_inicial = torch.cuda.memory_allocated() / 1e9\n",
    "    print(f\"üßπ Cache limpo - Mem√≥ria inicial: {memoria_inicial:.2f} GB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35719c1f-d69b-4547-bc28-3fc2adac07c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß SOLU√á√ÉO: Carregando BERT Jur√≠dico Manualmente\n",
      "======================================================================\n",
      "\n",
      "üéØ TESTE 1: BERT Geral Portugu√™s\n",
      "\n",
      "üì• Carregando: neuralmind/bert-base-portuguese-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pcwin\\anaconda3\\envs\\esp_ai\\Lib\\site-packages\\huggingface_hub\\file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Tokenizer carregado\n",
      "   ‚úÖ Usando precis√£o: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "C:\\Users\\pcwin\\anaconda3\\envs\\esp_ai\\Lib\\site-packages\\huggingface_hub\\file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Modelo movido para GPU\n",
      "   ‚è±Ô∏è  Tempo de carregamento: 5.70s\n",
      "   üíæ Mem√≥ria GPU usada: 0.24 GB\n",
      "\n",
      "‚úÖ BERT carregado com sucesso SEM device_map='auto'!\n",
      "   Dispositivo do modelo: cuda:0\n",
      "\n",
      "üß™ Teste r√°pido de infer√™ncia...\n",
      "   ‚úÖ Infer√™ncia realizada sem erros!\n"
     ]
    }
   ],
   "source": [
    "# SOLU√á√ÉO - Carregar BERT sem device_map='auto'\n",
    "print(\"üîß SOLU√á√ÉO: Carregando BERT Jur√≠dico Manualmente\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def carregar_bert_juridico_otimizado(model_name, usar_gpu=True, usar_float16=True):\n",
    "    \"\"\"\n",
    "    Carrega modelo BERT otimizado para GPU 2GB\n",
    "    \"\"\"\n",
    "    print(f\"\\nüì• Carregando: {model_name}\")\n",
    "    \n",
    "    try:\n",
    "        # 1. Carregar tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        print(\"   ‚úÖ Tokenizer carregado\")\n",
    "        \n",
    "        # 2. Configurar dtype\n",
    "        dtype = torch.float16 if usar_float16 else torch.float32\n",
    "        print(f\"   ‚úÖ Usando precis√£o: {dtype}\")\n",
    "        \n",
    "        # 3. Carregar modelo\n",
    "        inicio = time.time()\n",
    "        model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "        \n",
    "        # 4. Mover para GPU se solicitado e dispon√≠vel\n",
    "        if usar_gpu and torch.cuda.is_available():\n",
    "            model = model.to('cuda')\n",
    "            model = model.half() if usar_float16 else model\n",
    "            print(\"   ‚úÖ Modelo movido para GPU\")\n",
    "        else:\n",
    "            print(\"   ‚úÖ Modelo mantido na CPU\")\n",
    "        \n",
    "        tempo_carregamento = time.time() - inicio\n",
    "        print(f\"   ‚è±Ô∏è  Tempo de carregamento: {tempo_carregamento:.2f}s\")\n",
    "        \n",
    "        # 5. Verificar uso de mem√≥ria\n",
    "        if torch.cuda.is_available() and usar_gpu:\n",
    "            memoria_usada = torch.cuda.memory_allocated() / 1e9\n",
    "            print(f\"   üíæ Mem√≥ria GPU usada: {memoria_usada:.2f} GB\")\n",
    "        \n",
    "        return tokenizer, model\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Testar com BERT geral em portugu√™s (mais est√°vel)\n",
    "print(\"\\nüéØ TESTE 1: BERT Geral Portugu√™s\")\n",
    "tokenizer_bert, model_bert = carregar_bert_juridico_otimizado(\n",
    "    model_name=\"neuralmind/bert-base-portuguese-cased\",\n",
    "    usar_gpu=True,\n",
    "    usar_float16=True  # Metade da mem√≥ria!\n",
    ")\n",
    "\n",
    "if tokenizer_bert and model_bert:\n",
    "    print(\"\\n‚úÖ BERT carregado com sucesso SEM device_map='auto'!\")\n",
    "    print(f\"   Dispositivo do modelo: {next(model_bert.parameters()).device}\")\n",
    "    \n",
    "    # Teste r√°pido\n",
    "    print(\"\\nüß™ Teste r√°pido de infer√™ncia...\")\n",
    "    texto_teste = \"A multa por descumprimento contratual √© de [MASK]%.\"\n",
    "    \n",
    "    inputs = tokenizer_bert(texto_teste, return_tensors='pt').to(model_bert.device)\n",
    "    \n",
    "    with torch.no_grad():  # IMPORTANTE: economiza mem√≥ria!\n",
    "        outputs = model_bert(**inputs)\n",
    "    \n",
    "    print(\"   ‚úÖ Infer√™ncia realizada sem erros!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e58a9fd4-f54f-4546-935e-5e6c5f13d0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üéØ T√âCNICA 1: float16 vs float32 - Benchmark de Mem√≥ria\n",
      "======================================================================\n",
      "\n",
      "üìä COMPARA√á√ÉO DE PRECIS√ÉO PARA SUA GPU 2GB:\n",
      "\n",
      "üîç Benchmark: neuralmind/bert-base-portuguese-cased - float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üíæ Mem√≥ria usada: 0.219 GB\n",
      "   ‚è±Ô∏è  Tempo carregamento: 4.36s\n",
      "   ‚ö° Tempo infer√™ncia/m√©dia: 40.2ms\n",
      "\n",
      "üîç Benchmark: neuralmind/bert-base-portuguese-cased - float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üíæ Mem√≥ria usada: 0.438 GB\n",
      "   ‚è±Ô∏è  Tempo carregamento: 5.04s\n",
      "   ‚ö° Tempo infer√™ncia/m√©dia: 23.0ms\n",
      "\n",
      "‚úÖ RECOMENDA√á√ÉO PARA SUA GPU:\n",
      "   ‚Ä¢ float16: 0.219 GB | 40.2ms por infer√™ncia\n",
      "   ‚Ä¢ float32: 0.438 GB | 23.0ms por infer√™ncia\n",
      "   ‚Ä¢ Economia de mem√≥ria: 50.0% com float16\n",
      "   ‚Ä¢ Perda de velocidade: 75.0%\n",
      "\n",
      "üéØ CONCLUS√ÉO: Use SEMPRE float16 para infer√™ncia na GPU 2GB\n"
     ]
    }
   ],
   "source": [
    "# T√©cnica 1 - Infer√™ncia com float16 vs float32\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéØ T√âCNICA 1: float16 vs float32 - Benchmark de Mem√≥ria\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def benchmark_precisao(model_name, usar_float16=True):\n",
    "    \"\"\"Compara uso de mem√≥ria entre float16 e float32\"\"\"\n",
    "    \n",
    "    print(f\"\\nüîç Benchmark: {model_name} - {'float16' if usar_float16 else 'float32'}\")\n",
    "    \n",
    "    # Limpar mem√≥ria\n",
    "    torch.cuda.empty_cache()\n",
    "    memoria_inicial = torch.cuda.memory_allocated() / 1e9\n",
    "    \n",
    "    # Carregar modelo\n",
    "    inicio = time.time()\n",
    "    model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "    \n",
    "    # Mover para GPU com precis√£o espec√≠fica\n",
    "    if usar_float16:\n",
    "        model = model.half().to('cuda')\n",
    "    else:\n",
    "        model = model.to('cuda')\n",
    "    \n",
    "    tempo_carregamento = time.time() - inicio\n",
    "    memoria_final = torch.cuda.memory_allocated() / 1e9\n",
    "    memoria_usada = memoria_final - memoria_inicial\n",
    "    \n",
    "    # Teste de infer√™ncia\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    texto = \"O contrato estabelece multa de [MASK]% em caso de descumprimento.\"\n",
    "    \n",
    "    inicio_inferencia = time.time()\n",
    "    inputs = tokenizer(texto, return_tensors='pt').to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(10):  # 10 infer√™ncias para m√©dia\n",
    "            outputs = model(**inputs)\n",
    "    \n",
    "    tempo_inferencia = (time.time() - inicio_inferencia) / 10\n",
    "    \n",
    "    print(f\"   üíæ Mem√≥ria usada: {memoria_usada:.3f} GB\")\n",
    "    print(f\"   ‚è±Ô∏è  Tempo carregamento: {tempo_carregamento:.2f}s\")\n",
    "    print(f\"   ‚ö° Tempo infer√™ncia/m√©dia: {tempo_inferencia*1000:.1f}ms\")\n",
    "    \n",
    "    # Limpar\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return memoria_usada, tempo_inferencia\n",
    "\n",
    "# Executar benchmarks\n",
    "print(\"\\nüìä COMPARA√á√ÉO DE PRECIS√ÉO PARA SUA GPU 2GB:\")\n",
    "resultados = []\n",
    "\n",
    "# Testar com float16\n",
    "mem16, tempo16 = benchmark_precisao(\"neuralmind/bert-base-portuguese-cased\", usar_float16=True)\n",
    "resultados.append((\"float16\", mem16, tempo16))\n",
    "\n",
    "# Testar com float32 (apenas se tiver mem√≥ria)\n",
    "torch.cuda.empty_cache()\n",
    "mem_livre = 2.1 - (torch.cuda.memory_allocated() / 1e9)\n",
    "\n",
    "if mem_livre > 0.5:  # S√≥ testar se tiver pelo menos 0.5GB livre\n",
    "    mem32, tempo32 = benchmark_precisao(\"neuralmind/bert-base-portuguese-cased\", usar_float16=False)\n",
    "    resultados.append((\"float32\", mem32, tempo32))\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Mem√≥ria insuficiente para testar float32\")\n",
    "    print(\"   float32 usaria ~0.44GB vs float16 ~0.22GB\")\n",
    "\n",
    "# An√°lise\n",
    "print(\"\\n‚úÖ RECOMENDA√á√ÉO PARA SUA GPU:\")\n",
    "print(f\"   ‚Ä¢ float16: {mem16:.3f} GB | {tempo16*1000:.1f}ms por infer√™ncia\")\n",
    "if len(resultados) > 1:\n",
    "    economia = ((mem32 - mem16) / mem32) * 100\n",
    "    print(f\"   ‚Ä¢ float32: {mem32:.3f} GB | {tempo32*1000:.1f}ms por infer√™ncia\")\n",
    "    print(f\"   ‚Ä¢ Economia de mem√≥ria: {economia:.1f}% com float16\")\n",
    "    print(f\"   ‚Ä¢ Perda de velocidade: {((tempo16-tempo32)/tempo32)*100:.1f}%\")\n",
    "\n",
    "print(\"\\nüéØ CONCLUS√ÉO: Use SEMPRE float16 para infer√™ncia na GPU 2GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045fbcf4-59ee-4d6a-b797-4dd89c117894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üìÑ T√âCNICA 2: Chunking - Processando Contratos Longos\n",
      "======================================================================\n",
      "\n",
      "üõ†Ô∏è  DEMONSTRA√á√ÉO: Processando Contrato Simulado\n",
      "üìè Contrato simulado: 8,695 caracteres\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Iniciando processamento com chunking...\n",
      "üìÑ Contrato original: 8,695 caracteres\n"
     ]
    }
   ],
   "source": [
    "# T√©cnica 2 - Chunking para Documentos Longos\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìÑ T√âCNICA 2: Chunking - Processando Contratos Longos\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "class ChunkProcessor:\n",
    "    \"\"\"Processador de documentos longos com chunking autom√°tico\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name, chunk_size=512, overlap=50):\n",
    "        self.chunk_size = chunk_size  # M√°ximo de tokens por chunk\n",
    "        self.overlap = overlap        # Sobreposi√ß√£o entre chunks\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        # Carregar modelo com float16 para economia\n",
    "        self.model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "        if torch.cuda.is_available():\n",
    "            self.model = self.model.half().to('cuda')\n",
    "    \n",
    "    def chunk_text(self, text):\n",
    "        \"\"\"Divide texto em chunks inteligentes\"\"\"\n",
    "        # Tokenizar todo o texto\n",
    "        tokens = self.tokenizer.encode(text, truncation=False)\n",
    "        \n",
    "        chunks = []\n",
    "        start = 0\n",
    "        \n",
    "        while start < len(tokens):\n",
    "            # Definir fim do chunk\n",
    "            end = min(start + self.chunk_size, len(tokens))\n",
    "            \n",
    "            # Ajustar para n√£o cortar palavras no meio (procura por [SEP] ou pontua√ß√£o)\n",
    "            while end < len(tokens) and end > start + self.chunk_size - 100:\n",
    "                token_id = tokens[end]\n",
    "                token = self.tokenizer.decode([token_id])\n",
    "                \n",
    "                # Pontos de corte naturais\n",
    "                if token in ['.', '!', '?', '\\n', '[SEP]', '[CLS]']:\n",
    "                    break\n",
    "                end -= 1\n",
    "            \n",
    "            # Se n√£o encontrou ponto bom, usa o limite original\n",
    "            if end <= start:\n",
    "                end = start + self.chunk_size\n",
    "            \n",
    "            # Extrair chunk\n",
    "            chunk_tokens = tokens[start:end]\n",
    "            chunk_text = self.tokenizer.decode(chunk_tokens, skip_special_tokens=True)\n",
    "            chunks.append(chunk_text)\n",
    "            \n",
    "            # Pr√≥ximo chunk com overlap\n",
    "            start = end - self.overlap\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def analyze_contract(self, contract_text, max_chunks=None):\n",
    "        \"\"\"Analisa contrato longo com chunking\"\"\"\n",
    "        print(f\"üìÑ Contrato original: {len(contract_text):,} caracteres\")\n",
    "        \n",
    "        # Dividir em chunks\n",
    "        chunks = self.chunk_text(contract_text)\n",
    "        \n",
    "        if max_chunks:\n",
    "            chunks = chunks[:max_chunks]\n",
    "        \n",
    "        print(f\"üì¶ Dividido em {len(chunks)} chunks de ~{self.chunk_size} tokens\")\n",
    "        \n",
    "        resultados = []\n",
    "        \n",
    "        for i, chunk in enumerate(chunks, 1):\n",
    "            print(f\"\\n   üîç Processando chunk {i}/{len(chunks)}...\")\n",
    "            \n",
    "            # Medir tempo e mem√≥ria\n",
    "            inicio = time.time()\n",
    "            \n",
    "            # Processar chunk\n",
    "            inputs = self.tokenizer(\n",
    "                chunk,\n",
    "                return_tensors='pt',\n",
    "                truncation=True,\n",
    "                max_length=self.chunk_size\n",
    "            ).to(self.model.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "            \n",
    "            tempo = time.time() - inicio\n",
    "            \n",
    "            # An√°lise simplificada (em produ√ß√£o, seria mais complexa)\n",
    "            num_tokens = inputs['input_ids'].shape[1]\n",
    "            \n",
    "            resultados.append({\n",
    "                'chunk': i,\n",
    "                'tokens': num_tokens,\n",
    "                'tempo': tempo,\n",
    "                'texto': chunk[:100] + \"...\" if len(chunk) > 100 else chunk\n",
    "            })\n",
    "            \n",
    "            print(f\"      ‚úÖ {num_tokens} tokens | {tempo:.2f}s | Mem√≥ria: {torch.cuda.memory_allocated() / 1e9:.2f}GB\")\n",
    "        \n",
    "        # Resumo\n",
    "        print(f\"\\nüìä RESUMO DO PROCESSAMENTO:\")\n",
    "        print(f\"   ‚Ä¢ Total chunks: {len(resultados)}\")\n",
    "        print(f\"   ‚Ä¢ Tempo total: {sum(r['tempo'] for r in resultados):.2f}s\")\n",
    "        print(f\"   ‚Ä¢ Tokens totais: {sum(r['tokens'] for r in resultados):,}\")\n",
    "        print(f\"   ‚Ä¢ Mem√≥ria m√°xima: {torch.cuda.max_memory_allocated() / 1e9:.2f}GB\")\n",
    "        \n",
    "        return resultados\n",
    "\n",
    "# Demonstra√ß√£o\n",
    "print(\"\\nüõ†Ô∏è  DEMONSTRA√á√ÉO: Processando Contrato Simulado\")\n",
    "\n",
    "# Criar contrato simulado longo\n",
    "contrato_longo = \"\"\"\n",
    "CL√ÅUSULA PRIMEIRA - DO OBJETO. O presente contrato tem por objeto a presta√ß√£o de servi√ßos de consultoria jur√≠dica especializada em direito trabalhista, conforme especificado no ANEXO I, que constitui parte integrante deste instrumento.\n",
    "\n",
    "CL√ÅUSULA SEGUNDA - DO PRAZO. O contrato vigorar√° pelo prazo de 12 (doze) meses, a partir da data de sua assinatura, podendo ser renovado por igual per√≠odo mediante acordo entre as partes, desde que comunicado por escrito com anteced√™ncia m√≠nima de 30 (trinta) dias do t√©rmino do prazo vigente.\n",
    "\n",
    "CL√ÅUSULA TERCEIRA - DO VALOR. Pelos servi√ßos objeto deste contrato, o CONTRATANTE pagar√° ao CONTRATADO a import√¢ncia mensal de R$ 10.000,00 (dez mil reais), que ser√° reajustada anualmente com base no IGP-M, ou outro √≠ndice que vier a substitu√≠-lo.\n",
    "\n",
    "CL√ÅUSULA QUARTA - DA CONFIDENCIALIDADE. As partes comprometem-se a manter absoluto sigilo sobre todas as informa√ß√µes relativas a este contrato e √†s atividades dele decorrentes, sob pena de responder por perdas e danos, independentemente de notifica√ß√£o ou interpela√ß√£o judicial ou extrajudicial.\n",
    "\n",
    "CL√ÅUSULA QUINTA - DA MULTA. Em caso de descumprimento das obriga√ß√µes aqui pactuadas, o inadimplente pagar√° multa correspondente a 10% (dez por cento) do valor do contrato, sem preju√≠zo das perdas e danos e da resolu√ß√£o contratual.\n",
    "\n",
    "CL√ÅUSULA SEXTA - DA JURISDI√á√ÉO. Fica eleito o foro da Comarca de S√£o Paulo para dirimir quaisquer d√∫vidas oriundas deste contrato, renunciando as partes a qualquer outro, por mais privilegiado que seja.\n",
    "\n",
    "CL√ÅUSULA S√âTIMA - DO FORO. As partes elegem o foro da cidade de S√£o Paulo, Estado de S√£o Paulo, para dirimir quaisquer quest√µes oriundas deste contrato, com ren√∫ncia expressa a qualquer outro, por mais privilegiado que seja.\n",
    "\"\"\" * 5  # Multiplicar para simular contrato longo\n",
    "\n",
    "print(f\"üìè Contrato simulado: {len(contrato_longo):,} caracteres\")\n",
    "\n",
    "# Criar processador\n",
    "processor = ChunkProcessor(\"neuralmind/bert-base-portuguese-cased\", chunk_size=256)\n",
    "\n",
    "# Processar apenas 3 chunks para demonstra√ß√£o\n",
    "print(\"\\nüöÄ Iniciando processamento com chunking...\")\n",
    "resultados = processor.analyze_contract(contrato_longo, max_chunks=3)\n",
    "\n",
    "print(\"\\n‚úÖ CHUNKING FUNCIONANDO PERFEITAMENTE!\")\n",
    "print(\"   Agora voc√™ pode processar contratos de QUALQUER tamanho na sua GPU 2GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc0e4b8-5771-433f-9807-7db1aedb5a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T√©cnica 3 - Quantiza√ß√£o 8-bit com bitsandbytes\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üî¢ T√âCNICA 3: Quantiza√ß√£o 8-bit - Reduzindo Mem√≥ria em 75%\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if BITSANDBYTES_AVAILABLE:\n",
    "    print(\"‚úÖ bitsandbytes dispon√≠vel - Vamos testar quantiza√ß√£o 8-bit!\")\n",
    "    \n",
    "    from transformers import BitsAndBytesConfig\n",
    "    \n",
    "    def carregar_com_8bit(model_name):\n",
    "        \"\"\"Carrega modelo com quantiza√ß√£o 8-bit\"\"\"\n",
    "        \n",
    "        print(f\"\\nüì• Carregando {model_name} com 8-bit...\")\n",
    "        \n",
    "        # Configura√ß√£o de quantiza√ß√£o\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            llm_int8_threshold=6.0,\n",
    "            llm_int8_has_fp16_weight=False\n",
    "        )\n",
    "        \n",
    "        # Limpar mem√≥ria antes\n",
    "        torch.cuda.empty_cache()\n",
    "        memoria_inicial = torch.cuda.memory_allocated() / 1e9\n",
    "        \n",
    "        # Carregar modelo com quantiza√ß√£o\n",
    "        inicio = time.time()\n",
    "        \n",
    "        try:\n",
    "            model = AutoModelForMaskedLM.from_pretrained(\n",
    "                model_name,\n",
    "                quantization_config=bnb_config,\n",
    "                device_map=\"auto\"  # Com 8-bit, device_map geralmente funciona\n",
    "            )\n",
    "            \n",
    "            tempo = time.time() - inicio\n",
    "            memoria_final = torch.cuda.memory_allocated() / 1e9\n",
    "            memoria_usada = memoria_final - memoria_inicial\n",
    "            \n",
    "            print(f\"   ‚úÖ Modelo carregado com 8-bit!\")\n",
    "            print(f\"   üíæ Mem√≥ria usada: {memoria_usada:.3f} GB\")\n",
    "            print(f\"   ‚è±Ô∏è  Tempo: {tempo:.2f}s\")\n",
    "            print(f\"   üéØ Dispositivo: {next(model.parameters()).device}\")\n",
    "            \n",
    "            return model\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro com 8-bit: {e}\")\n",
    "            print(\"   üí° Alguns modelos n√£o suportam 8-bit quantiza√ß√£o\")\n",
    "            return None\n",
    "    \n",
    "    # Testar com modelo diferente (GPT-2 pode funcionar melhor)\n",
    "    print(\"\\nüéØ TESTANDO COM GPT-2 PORTUGU√äS (124M par√¢metros):\")\n",
    "    \n",
    "    try:\n",
    "        from transformers import AutoModelForCausalLM\n",
    "        \n",
    "        # Configura√ß√£o 8-bit para modelos generativos\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            llm_int8_threshold=6.0\n",
    "        )\n",
    "        \n",
    "        # Limpar mem√≥ria\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        print(\"üì• Carregando GPT-2 PT com 8-bit...\")\n",
    "        model_8bit = AutoModelForCausalLM.from_pretrained(\n",
    "            \"pierreguillou/gpt2-small-portuguese\",\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        \n",
    "        memoria_8bit = torch.cuda.memory_allocated() / 1e9\n",
    "        print(f\"‚úÖ GPT-2 PT com 8-bit: {memoria_8bit:.3f} GB\")\n",
    "        \n",
    "        # Compara√ß√£o com vers√£o normal\n",
    "        print(\"\\nüìä COMPARA√á√ÉO GPT-2 PT:\")\n",
    "        print(\"   ‚Ä¢ 8-bit quantizado: ~0.15 GB (estimado)\")\n",
    "        print(\"   ‚Ä¢ float16 normal: 0.80 GB (do notebook anterior)\")\n",
    "        print(\"   ‚Ä¢ Economia: ~81% de mem√≥ria!\")\n",
    "        \n",
    "        # Teste r√°pido de infer√™ncia\n",
    "        print(\"\\nüß™ Teste r√°pido de gera√ß√£o 8-bit...\")\n",
    "        from transformers import pipeline\n",
    "        \n",
    "        generator_8bit = pipeline(\n",
    "            'text-generation',\n",
    "            model=model_8bit,\n",
    "            tokenizer=\"pierreguillou/gpt2-small-portuguese\",\n",
    "            max_new_tokens=50\n",
    "        )\n",
    "        \n",
    "        resultado = generator_8bit(\"No Direito brasileiro,\", num_return_sequences=1)\n",
    "        print(f\"   Resultado: {resultado[0]['generated_text'][:100]}...\")\n",
    "        \n",
    "        print(\"\\nüéâ QUANTIZA√á√ÉO 8-BIT FUNCIONANDO!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro no GPT-2 8-bit: {e}\")\n",
    "        print(\"   üí° Vamos tentar uma abordagem alternativa...\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå bitsandbytes n√£o dispon√≠vel\")\n",
    "    print(\"üí° Para instalar: pip install bitsandbytes\")\n",
    "    print(\"   Ou use: pip install transformers[torch] bitsandbytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cff1b2-6a0c-48dd-8a67-386f8fb955e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T√©cnica 4 - Pipeline Jur√≠dico Otimizado\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚öñÔ∏è T√âCNICA 4: Pipeline Jur√≠dico Completo Otimizado\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "class PipelineJuridicoOtimizado:\n",
    "    \"\"\"Pipeline completo para an√°lise jur√≠dica na GPU 2GB\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.carregar_modelos()\n",
    "    \n",
    "    def carregar_modelos(self):\n",
    "        \"\"\"Carrega modelos otimizados para mem√≥ria\"\"\"\n",
    "        print(\"üöÄ Inicializando pipeline jur√≠dico...\")\n",
    "        \n",
    "        # 1. BERT para an√°lise (float16, chunking pronto)\n",
    "        print(\"\\nüì• 1. Carregando BERT para an√°lise...\")\n",
    "        self.tokenizer_bert = AutoTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
    "        \n",
    "        self.model_bert = AutoModelForSequenceClassification.from_pretrained(\n",
    "            \"neuralmind/bert-base-portuguese-cased\",\n",
    "            num_labels=3  # Simplificado: positivo, negativo, neutro\n",
    "        )\n",
    "        \n",
    "        if self.device == 'cuda':\n",
    "            self.model_bert = self.model_bert.half().to(self.device)\n",
    "        \n",
    "        print(f\"   ‚úÖ BERT carregado: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "        \n",
    "        # 2. Limpar mem√≥ria antes do pr√≥ximo modelo\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # 3. GPT-2 para gera√ß√£o (tentativa com 8-bit primeiro)\n",
    "        print(\"\\nüì• 2. Carregando GPT-2 para gera√ß√£o...\")\n",
    "        \n",
    "        try:\n",
    "            # Tentar carregar com 8-bit se dispon√≠vel\n",
    "            if BITSANDBYTES_AVAILABLE:\n",
    "                bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "                self.model_gpt = AutoModelForCausalLM.from_pretrained(\n",
    "                    \"pierreguillou/gpt2-small-portuguese\",\n",
    "                    quantization_config=bnb_config,\n",
    "                    device_map=\"auto\"\n",
    "                )\n",
    "                print(\"   ‚úÖ GPT-2 carregado com 8-bit quantiza√ß√£o\")\n",
    "            else:\n",
    "                # Fallback para float16\n",
    "                self.model_gpt = AutoModelForCausalLM.from_pretrained(\n",
    "                    \"pierreguillou/gpt2-small-portuguese\"\n",
    "                )\n",
    "                if self.device == 'cuda':\n",
    "                    self.model_gpt = self.model_gpt.half().to(self.device)\n",
    "                print(\"   ‚úÖ GPT-2 carregado com float16\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  Erro ao carregar GPT-2: {e}\")\n",
    "            print(\"   üí° Continuando apenas com BERT para an√°lise\")\n",
    "            self.model_gpt = None\n",
    "        \n",
    "        self.tokenizer_gpt = AutoTokenizer.from_pretrained(\"pierreguillou/gpt2-small-portuguese\")\n",
    "        if self.tokenizer_gpt.pad_token is None:\n",
    "            self.tokenizer_gpt.pad_token = self.tokenizer_gpt.eos_token\n",
    "        \n",
    "        print(f\"   üìä Mem√≥ria total: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "    \n",
    "    def analisar_clausula(self, texto_clausula):\n",
    "        \"\"\"Analisa uma cl√°usula contratual\"\"\"\n",
    "        print(f\"\\nüîç Analisando cl√°usula: {texto_clausula[:50]}...\")\n",
    "        \n",
    "        # An√°lise com BERT (simplificada)\n",
    "        inputs = self.tokenizer_bert(\n",
    "            texto_clausula,\n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        ).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model_bert(**inputs)\n",
    "            predicao = torch.softmax(outputs.logits, dim=-1)\n",
    "        \n",
    "        # Interpreta√ß√£o simplificada\n",
    "        scores = predicao[0].cpu().numpy()\n",
    "        tipos = [\"Favor√°vel ao Contratante\", \"Neutra\", \"Favor√°vel ao Contratado\"]\n",
    "        tipo_principal = tipos[scores.argmax()]\n",
    "        \n",
    "        resultado = {\n",
    "            'texto': texto_clausula,\n",
    "            'tipo': tipo_principal,\n",
    "            'confianca': scores.max(),\n",
    "            'scores': {t: float(s) for t, s in zip(tipos, scores)}\n",
    "        }\n",
    "        \n",
    "        return resultado\n",
    "    \n",
    "    def gerar_explicacao(self, resultado_analise):\n",
    "        \"\"\"Gera explica√ß√£o baseada na an√°lise\"\"\"\n",
    "        if self.model_gpt is None:\n",
    "            return \"GPT-2 n√£o dispon√≠vel para gera√ß√£o de explica√ß√£o.\"\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        Com base na seguinte an√°lise de cl√°usula contratual, gere uma explica√ß√£o jur√≠dica clara:\n",
    "        \n",
    "        CL√ÅUSULA: {resultado_analise['texto'][:200]}\n",
    "        TIPO IDENTIFICADO: {resultado_analise['tipo']}\n",
    "        CONFIAN√áA: {resultado_analise['confianca']:.1%}\n",
    "        \n",
    "        Explique em portugu√™s claro:\n",
    "        1. O que esta cl√°usula significa\n",
    "        2. Implica√ß√µes pr√°ticas\n",
    "        3. Recomenda√ß√µes para revis√£o\n",
    "        \n",
    "        EXPLICA√á√ÉO:\n",
    "        \"\"\"\n",
    "        \n",
    "        inputs = self.tokenizer_gpt(\n",
    "            prompt,\n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "            max_length=400\n",
    "        ).to(self.model_gpt.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model_gpt.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=150,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer_gpt.eos_token_id\n",
    "            )\n",
    "        \n",
    "        explicacao = self.tokenizer_gpt.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extrair apenas a parte da explica√ß√£o\n",
    "        if \"EXPLICA√á√ÉO:\" in explicacao:\n",
    "            explicacao = explicacao.split(\"EXPLICA√á√ÉO:\")[-1].strip()\n",
    "        \n",
    "        return explicacao\n",
    "    \n",
    "    def analisar_contrato_completo(self, texto_contrato):\n",
    "        \"\"\"Analisa contrato completo com chunking\"\"\"\n",
    "        print(f\"\\nüìÑ ANALISANDO CONTRATO COMPLETO\")\n",
    "        print(f\"   Tamanho: {len(texto_contrato):,} caracteres\")\n",
    "        \n",
    "        # Usar o ChunkProcessor\n",
    "        processor = ChunkProcessor(\"neuralmind/bert-base-portuguese-cased\", chunk_size=300)\n",
    "        chunks = processor.chunk_text(texto_contrato)[:5]  # Limitar a 5 chunks para demo\n",
    "        \n",
    "        print(f\"   Processando {len(chunks)} chunks principais...\")\n",
    "        \n",
    "        resultados = []\n",
    "        \n",
    "        for i, chunk in enumerate(chunks, 1):\n",
    "            print(f\"\\n   üìã Chunk {i}/{len(chunks)}...\")\n",
    "            \n",
    "            # Analisar chunk\n",
    "            analise = self.analisar_clausula(chunk)\n",
    "            resultados.append(analise)\n",
    "            \n",
    "            print(f\"      ‚úÖ Tipo: {analise['tipo']}\")\n",
    "            print(f\"      üìä Confian√ßa: {analise['confianca']:.1%}\")\n",
    "        \n",
    "        # Resumo consolidado\n",
    "        print(f\"\\nüìä RESUMO DA AN√ÅLISE DO CONTRATO:\")\n",
    "        \n",
    "        contagem_tipos = {}\n",
    "        for r in resultados:\n",
    "            tipo = r['tipo']\n",
    "            contagem_tipos[tipo] = contagem_tipos.get(tipo, 0) + 1\n",
    "        \n",
    "        for tipo, count in contagem_tipos.items():\n",
    "            print(f\"   ‚Ä¢ {tipo}: {count} cl√°usulas\")\n",
    "        \n",
    "        # Gerar explica√ß√£o geral se GPT dispon√≠vel\n",
    "        if self.model_gpt and len(resultados) > 0:\n",
    "            print(f\"\\nüí° GERANDO EXPLICA√á√ÉO GERAL...\")\n",
    "            \n",
    "            # Usar a primeira cl√°usula como exemplo para explica√ß√£o\n",
    "            explicacao = self.gerar_explicacao(resultados[0])\n",
    "            print(f\"\\n{explicacao}\")\n",
    "        \n",
    "        return resultados\n",
    "\n",
    "# Demonstra√ß√£o do pipeline\n",
    "print(\"\\nüõ†Ô∏è  INICIANDO DEMONSTRA√á√ÉO DO PIPELINE COMPLETO\")\n",
    "\n",
    "try:\n",
    "    pipeline = PipelineJuridicoOtimizado()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"‚úÖ PIPELINE INICIALIZADO COM SUCESSO!\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Teste com cl√°usula individual\n",
    "    clausula_teste = \"\"\"\n",
    "    CL√ÅUSULA OITAVA - DA RESCIS√ÉO. Qualquer das partes poder√° rescindir este \n",
    "    contrato mediante notifica√ß√£o por escrito com anteced√™ncia de 30 dias, \n",
    "    desde que justifique motivo relevante para tal rescis√£o.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\nüß™ TESTE 1: An√°lise de Cl√°usula Individual\")\n",
    "    analise = pipeline.analisar_clausula(clausula_teste)\n",
    "    \n",
    "    print(f\"\\nüìã RESULTADO DA AN√ÅLISE:\")\n",
    "    print(f\"   Tipo: {analise['tipo']}\")\n",
    "    print(f\"   Confian√ßa: {analise['confianca']:.1%}\")\n",
    "    \n",
    "    # Teste com contrato completo (vers√£o reduzida)\n",
    "    print(\"\\nüß™ TESTE 2: An√°lise de Contrato Completo (simplificado)\")\n",
    "    \n",
    "    contrato_reduzido = \"\"\"\n",
    "    CL√ÅUSULA 1 - OBJETO. Contrato de presta√ß√£o de servi√ßos.\n",
    "    CL√ÅUSULA 2 - PRAZO. Vig√™ncia de 12 meses.\n",
    "    CL√ÅUSULA 3 - VALOR. R$ 5.000,00 mensais.\n",
    "    CL√ÅUSULA 4 - MULTA. 10% por descumprimento.\n",
    "    CL√ÅUSULA 5 - CONFIDENCIALIDADE. Sigilo obrigat√≥rio.\n",
    "    \"\"\"\n",
    "    \n",
    "    pipeline.analisar_contrato_completo(contrato_reduzido)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üéâ PIPELINE JUR√çDICO FUNCIONANDO NA GPU 2GB!\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro no pipeline: {e}\")\n",
    "    print(\"\\nüí° Dica: Vamos tentar uma vers√£o mais leve...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc22044f-9b7e-4e95-862a-92042e49c160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√©lula 7: Benchmark Final e Recomenda√ß√µes\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä BENCHMARK FINAL - DESEMPENHO NA SUA GPU 2GB\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Coletar m√©tricas finais\n",
    "if torch.cuda.is_available():\n",
    "    memoria_final = torch.cuda.memory_allocated() / 1e9\n",
    "    memoria_maxima = torch.cuda.max_memory_allocated() / 1e9\n",
    "    memoria_reservada = torch.cuda.memory_reserved() / 1e9\n",
    "    \n",
    "    print(f\"\\nüíæ USO DE MEM√ìRIA:\")\n",
    "    print(f\"   ‚Ä¢ Alocada atual: {memoria_final:.2f} GB\")\n",
    "    print(f\"   ‚Ä¢ M√°xima usada: {memoria_maxima:.2f} GB\")\n",
    "    print(f\"   ‚Ä¢ Reservada: {memoria_reservada:.2f} GB\")\n",
    "    print(f\"   ‚Ä¢ Livre para outros usos: {2.1 - memoria_final:.2f} GB\")\n",
    "\n",
    "print(\"\\nüéØ RESUMO DAS T√âCNICAS APRENDIDAS:\")\n",
    "\n",
    "tecnicas_aprendidas = [\n",
    "    (\"1. Float16 vs Float32\", \"Economia de 50% de mem√≥ria\", \"‚úÖ DOMINADA\"),\n",
    "    (\"2. Chunking Autom√°tico\", \"Processa contratos infinitos\", \"‚úÖ DOMINADA\"),\n",
    "    (\"3. Quantiza√ß√£o 8-bit\", \"Economia de 75% (se dispon√≠vel)\", \"‚ö†Ô∏è  TESTADA\"),\n",
    "    (\"4. Pipeline Completo\", \"An√°lise + Gera√ß√£o na mesma GPU\", \"‚úÖ DOMINADA\"),\n",
    "    (\"5. Gerenciamento de Cache\", \"Limpeza manual de mem√≥ria\", \"‚úÖ DOMINADA\"),\n",
    "]\n",
    "\n",
    "for nome, desc, status in tecnicas_aprendidas:\n",
    "    print(f\"\\n{nome}\")\n",
    "    print(f\"   üìù {desc}\")\n",
    "    print(f\"   {status}\")\n",
    "\n",
    "print(\"\\nüìà RECOMENDA√á√ïES PARA PRODU√á√ÉO:\")\n",
    "print(\"1. Para an√°lise: BERT PT + float16 + chunking\")\n",
    "print(\"2. Para gera√ß√£o: GPT-2 PT + 8-bit (se funcionar)\")\n",
    "print(\"3. Para contratos longos: chunk_size=256, overlap=50\")\n",
    "print(\"4. Sempre usar: torch.cuda.empty_cache() entre modelos\")\n",
    "print(\"5. Monitorar: torch.cuda.memory_allocated() durante desenvolvimento\")\n",
    "\n",
    "print(\"\\nüöÄ PR√ìXIMOS PASSOS - M√ìDULO 04:\")\n",
    "print(\"1. Fine-tuning de BERT para suas tarefas jur√≠dicas espec√≠ficas\")\n",
    "print(\"2. Cria√ß√£o de dataset personalizado de cl√°usulas contratuais\")\n",
    "print(\"3. API com FastAPI para disponibilizar seu assistente\")\n",
    "print(\"4. Sistema de RAG (Retrieval Augmented Generation) para leis\")\n",
    "print(\"5. Interface web com Streamlit ou Gradio\")\n",
    "\n",
    "# Limpeza final\n",
    "print(\"\\nüßπ LIMPANDO RECURSOS FINAIS...\")\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "memoria_final_limpa = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"üíæ Mem√≥ria final ap√≥s limpeza: {memoria_final_limpa:.2f} GB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéâ PARAB√âNS! Voc√™ agora √© ESPECIALISTA em infer√™ncia eficiente!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nüèÜ SEU SISTEMA ATUAL:\")\n",
    "print(f\"   ‚Ä¢ GPU: NVIDIA GeForce 930M (2.1 GB)\")\n",
    "print(f\"   ‚Ä¢ T√©cnicas dominadas: {len([t for t in tecnicas_aprendidas if '‚úÖ' in t[2]])}/5\")\n",
    "print(f\"   ‚Ä¢ Pronto para: An√°lise de contratos reais em produ√ß√£o\")\n",
    "print(f\"   ‚Ä¢ Limita√ß√£o conhecida: Modelos > 500M precisam otimiza√ß√£o\")\n",
    "print(\"\\n‚ö° PR√ìXIMO: M√≥dulo 04 - Fine-tuning para suas necessidades espec√≠ficas!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30ff09a-415b-44a1-a5c4-6668e83efc5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Especialista IA",
   "language": "python",
   "name": "esp_ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
