{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0054b74d-e5c3-47b8-857d-59d04ebf89f8",
   "metadata": {},
   "source": [
    "# âš¡ 03 - InferÃªncia Eficiente para GPU 2GB\n",
    "## ğŸ¯ Maximizando Performance no Hardware Limitado\n",
    "\n",
    "**Problema anterior resolvido:** Alguns modelos BERT nÃ£o suportam `device_map='auto'`\n",
    "**SoluÃ§Ã£o:** Carregamento manual otimizado + tÃ©cnicas de inferÃªncia eficiente\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Š RESULTADOS DO NOTEBOOK 02\n",
    "âœ… GPT-2 PT: 0.8GB | Funciona perfeitamente  \n",
    "âš ï¸ BERT Legal: Precisa carregamento manual (sem device_map)  \n",
    "ğŸ¯ MemÃ³ria livre: 1.3GB (Ã³timo para otimizaÃ§Ãµes)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ OBJETIVOS DESTE NOTEBOOK\n",
    "1. Carregar BERT jurÃ­dico SEM device_map='auto'\n",
    "2. Otimizar inferÃªncia com float16 e chunking\n",
    "3. Implementar cache de atenÃ§Ã£o para documentos longos\n",
    "4. Testar quantizaÃ§Ã£o 8-bit (se bitsandbytes instalado)\n",
    "5. Criar pipeline jurÃ­dico ULTRA eficiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38886a32-ba5b-4621-baf8-bd4797e917c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "âš¡ INFERÃŠNCIA LEVE - CONFIGURAÃ‡ÃƒO RÃPIDA\n",
      "======================================================================\n",
      "ğŸ”§ PyTorch: 2.3.1+cu118\n",
      "ğŸ® GPU: NVIDIA GeForce 930M\n",
      "ğŸ’¾ VRAM Total: 2.1 GB\n",
      "ğŸ§¹ Cache GPU limpo\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ConfiguraÃ§Ã£o RÃPIDA\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModelForSequenceClassification\n",
    "import time\n",
    "import os\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"âš¡ INFERÃŠNCIA LEVE - CONFIGURAÃ‡ÃƒO RÃPIDA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ForÃ§ar uso de cache local (evitar downloads)\n",
    "os.environ['TRANSFORMERS_OFFLINE'] = '0'  # Mas usa cache\n",
    "os.environ['HF_HUB_OFFLINE'] = '0'\n",
    "\n",
    "# Status rÃ¡pido\n",
    "print(f\"ğŸ”§ PyTorch: {torch.__version__}\")\n",
    "print(f\"ğŸ® GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
    "print(f\"ğŸ’¾ VRAM Total: 2.1 GB\")\n",
    "\n",
    "# Limpar para comeÃ§ar\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"ğŸ§¹ Cache GPU limpo\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2495f79-1d4a-4716-9126-ed1983b17e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥ CARREGAMENTO ÃšNICO DE MODELOS\n",
      "======================================================================\n",
      "\n",
      "ğŸ¯ CARREGANDO MODELOS ESSENCIAIS:\n",
      "   ğŸ“¥ Carregando neuralmind/bert-base-portuguese-cased...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pcwin\\anaconda3\\envs\\esp_ai\\Lib\\site-packages\\huggingface_hub\\file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Carregado em 12.5s | 0.23GB\n",
      "\n",
      "ğŸ“Š STATUS APÃ“S CARREGAMENTO:\n",
      "   ğŸ’¾ MemÃ³ria usada: 0.23 GB\n",
      "   ğŸ¯ MemÃ³ria livre: 1.87 GB\n"
     ]
    }
   ],
   "source": [
    "# CARREGAMENTO ÃšNICO (evita downloads repetidos)\n",
    "print(\"ğŸ“¥ CARREGAMENTO ÃšNICO DE MODELOS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# DicionÃ¡rio para armazenar modelos jÃ¡ carregados\n",
    "modelos_carregados = {}\n",
    "\n",
    "def carregar_modelo_uma_vez(nome_modelo, tipo='masked'):\n",
    "    \"\"\"\n",
    "    Carrega modelo UMA VEZ e reutiliza\n",
    "    \"\"\"\n",
    "    if nome_modelo in modelos_carregados:\n",
    "        print(f\"   âš¡ REUTILIZANDO {nome_modelo} (jÃ¡ carregado)\")\n",
    "        return modelos_carregados[nome_modelo]\n",
    "    \n",
    "    print(f\"   ğŸ“¥ Carregando {nome_modelo}...\")\n",
    "    inicio = time.time()\n",
    "    \n",
    "    # Carregar tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(nome_modelo)\n",
    "    \n",
    "    # Carregar modelo com tipo correto\n",
    "    if tipo == 'masked':\n",
    "        model = AutoModelForMaskedLM.from_pretrained(nome_modelo)\n",
    "    elif tipo == 'sequence':\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            nome_modelo,\n",
    "            num_labels=3  # Para classificaÃ§Ã£o simples\n",
    "        )\n",
    "    else:\n",
    "        model = AutoModelForMaskedLM.from_pretrained(nome_modelo)\n",
    "    \n",
    "    # Otimizar para GPU\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.half().to('cuda')  # float16 + GPU\n",
    "    \n",
    "    tempo = time.time() - inicio\n",
    "    memoria = torch.cuda.memory_allocated() / 1e9 if torch.cuda.is_available() else 0\n",
    "    \n",
    "    print(f\"   âœ… Carregado em {tempo:.1f}s | {memoria:.2f}GB\")\n",
    "    \n",
    "    # Armazenar para reutilizaÃ§Ã£o\n",
    "    modelos_carregados[nome_modelo] = (tokenizer, model)\n",
    "    \n",
    "    return tokenizer, model\n",
    "\n",
    "# Carregar APENAS OS ESSENCIAIS\n",
    "print(\"\\nğŸ¯ CARREGANDO MODELOS ESSENCIAIS:\")\n",
    "\n",
    "# 1. BERT para anÃ¡lise\n",
    "tokenizer_bert, model_bert = carregar_modelo_uma_vez(\n",
    "    \"neuralmind/bert-base-portuguese-cased\",\n",
    "    tipo='sequence'\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ“Š STATUS APÃ“S CARREGAMENTO:\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   ğŸ’¾ MemÃ³ria usada: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "    print(f\"   ğŸ¯ MemÃ³ria livre: {2.1 - (torch.cuda.memory_allocated() / 1e9):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0205eb72-2655-4af2-be10-1adc1f843e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ğŸ“„ CHUNKING SIMPLES (sem carregar novo modelo)\n",
      "======================================================================\n",
      "\n",
      "ğŸ§ª TESTE RÃPIDO DE CHUNKING:\n",
      "   Texto original: 151 caracteres\n",
      "   Dividido em: 1 chunks\n",
      "   Chunk 1: CLÃUSULA 1 - OBJETO. Contrato de serviÃ§os.\n",
      "CLÃUSUL...\n"
     ]
    }
   ],
   "source": [
    "# TÃ‰CNICA ESSENCIAL 1 - Chunking SIMPLES\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“„ CHUNKING SIMPLES (sem carregar novo modelo)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def chunking_simples(texto, max_tokens=256):\n",
    "    \"\"\"\n",
    "    Chunking simples sem carregar novo modelo\n",
    "    \"\"\"\n",
    "    # Dividir por parÃ¡grafos (simples e eficiente)\n",
    "    paragrafos = [p.strip() for p in texto.split('\\n\\n') if p.strip()]\n",
    "    \n",
    "    chunks = []\n",
    "    chunk_atual = \"\"\n",
    "    tokens_atual = 0\n",
    "    \n",
    "    for paragrafo in paragrafos:\n",
    "        # Estimativa simples de tokens (4 chars â‰ˆ 1 token)\n",
    "        tokens_paragrafo = len(paragrafo) // 4\n",
    "        \n",
    "        if tokens_atual + tokens_paragrafo > max_tokens and chunk_atual:\n",
    "            # Salvar chunk atual e comeÃ§ar novo\n",
    "            chunks.append(chunk_atual.strip())\n",
    "            chunk_atual = paragrafo\n",
    "            tokens_atual = tokens_paragrafo\n",
    "        else:\n",
    "            # Adicionar ao chunk atual\n",
    "            if chunk_atual:\n",
    "                chunk_atual += \"\\n\\n\" + paragrafo\n",
    "            else:\n",
    "                chunk_atual = paragrafo\n",
    "            tokens_atual += tokens_paragrafo\n",
    "    \n",
    "    # Adicionar Ãºltimo chunk\n",
    "    if chunk_atual:\n",
    "        chunks.append(chunk_atual.strip())\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Teste RÃPIDO\n",
    "print(\"\\nğŸ§ª TESTE RÃPIDO DE CHUNKING:\")\n",
    "\n",
    "contrato_pequeno = \"\"\"\n",
    "CLÃUSULA 1 - OBJETO. Contrato de serviÃ§os.\n",
    "CLÃUSULA 2 - PRAZO. 12 meses.\n",
    "CLÃUSULA 3 - VALOR. R$ 5.000,00.\n",
    "CLÃUSULA 4 - MULTA. 10% por descumprimento.\n",
    "\"\"\"\n",
    "\n",
    "chunks = chunking_simples(contrato_pequeno, max_tokens=50)\n",
    "print(f\"   Texto original: {len(contrato_pequeno)} caracteres\")\n",
    "print(f\"   Dividido em: {len(chunks)} chunks\")\n",
    "for i, chunk in enumerate(chunks, 1):\n",
    "    print(f\"   Chunk {i}: {chunk[:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8c3efc6-3d6d-4eaa-a228-269e5b4f4eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "âš–ï¸ PIPELINE JURÃDICO RÃPIDO (com modelos jÃ¡ carregados)\n",
      "======================================================================\n",
      "\n",
      "ğŸš€ CRIANDO PIPELINE RÃPIDO...\n",
      "\n",
      "ğŸ§ª TESTE 1: AnÃ¡lise de clÃ¡usula Ãºnica\n",
      "\n",
      "ğŸ“‹ RESULTADO:\n",
      "   Texto: Multa de 10% por atraso na entrega.\n",
      "   Tipo: Normal\n",
      "   ConfianÃ§a: 35.6%\n",
      "   Tempo: 682.2ms\n",
      "   Tokens: 12\n",
      "\n",
      "ğŸ§ª TESTE 2: AnÃ¡lise de contrato pequeno\n",
      "\n",
      "ğŸ“„ ANALISANDO CONTRATO: 258 chars\n",
      "   ğŸ“¦ Dividido em 1 chunks\n",
      "   ğŸ” Analisando chunk 1/1... âœ… CrÃ­tica (38%)\n",
      "\n",
      "ğŸ“Š RESUMO DA ANÃLISE:\n",
      "   â€¢ CrÃ­tica: 38% confianÃ§a\n",
      "\n",
      "âœ… PIPELINE FUNCIONANDO NA GPU 2GB!\n"
     ]
    }
   ],
   "source": [
    "# TÃ‰CNICA ESSENCIAL 2 - Pipeline JurÃ­dico RÃPIDO\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âš–ï¸ PIPELINE JURÃDICO RÃPIDO (com modelos jÃ¡ carregados)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "class PipelineJuridicoRapido:\n",
    "    \"\"\"Pipeline ultra leve usando modelos jÃ¡ carregados\"\"\"\n",
    "    \n",
    "    def __init__(self, tokenizer, model):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.device = next(model.parameters()).device\n",
    "        \n",
    "    def analisar_clausula_rapido(self, texto):\n",
    "        \"\"\"AnÃ¡lise RÃPIDA de uma clÃ¡usula\"\"\"\n",
    "        # Tokenizar\n",
    "        inputs = self.tokenizer(\n",
    "            texto,\n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "            max_length=256,\n",
    "            padding=True\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # InferÃªncia RÃPIDA\n",
    "        inicio = time.time()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            predicao = torch.softmax(outputs.logits, dim=-1)\n",
    "        \n",
    "        tempo = time.time() - inicio\n",
    "        \n",
    "        # Resultado simplificado\n",
    "        scores = predicao[0].cpu().numpy()\n",
    "        tipos = [\"Normal\", \"Importante\", \"CrÃ­tica\"]\n",
    "        tipo_idx = scores.argmax()\n",
    "        \n",
    "        return {\n",
    "            'texto': texto[:100] + \"...\" if len(texto) > 100 else texto,\n",
    "            'tipo': tipos[tipo_idx],\n",
    "            'confianca': float(scores[tipo_idx]),\n",
    "            'tempo_ms': tempo * 1000,\n",
    "            'tokens': inputs['input_ids'].shape[1]\n",
    "        }\n",
    "    \n",
    "    def analisar_contrato_rapido(self, texto_contrato):\n",
    "        \"\"\"AnÃ¡lise RÃPIDA de contrato completo\"\"\"\n",
    "        print(f\"\\nğŸ“„ ANALISANDO CONTRATO: {len(texto_contrato):,} chars\")\n",
    "        \n",
    "        # 1. Chunking simples\n",
    "        chunks = chunking_simples(texto_contrato, max_tokens=200)\n",
    "        print(f\"   ğŸ“¦ Dividido em {len(chunks)} chunks\")\n",
    "        \n",
    "        # 2. Analisar cada chunk (limitar a 3 para velocidade)\n",
    "        chunks = chunks[:3]  # LIMITE para nÃ£o travar\n",
    "        resultados = []\n",
    "        \n",
    "        for i, chunk in enumerate(chunks, 1):\n",
    "            print(f\"   ğŸ” Analisando chunk {i}/{len(chunks)}...\", end=\"\")\n",
    "            \n",
    "            resultado = self.analisar_clausula_rapido(chunk)\n",
    "            resultados.append(resultado)\n",
    "            \n",
    "            print(f\" âœ… {resultado['tipo']} ({resultado['confianca']:.0%})\")\n",
    "        \n",
    "        # 3. Resumo\n",
    "        print(f\"\\nğŸ“Š RESUMO DA ANÃLISE:\")\n",
    "        for r in resultados:\n",
    "            print(f\"   â€¢ {r['tipo']}: {r['confianca']:.0%} confianÃ§a\")\n",
    "        \n",
    "        return resultados\n",
    "\n",
    "# Criar pipeline RÃPIDO\n",
    "print(\"\\nğŸš€ CRIANDO PIPELINE RÃPIDO...\")\n",
    "pipeline = PipelineJuridicoRapido(tokenizer_bert, model_bert)\n",
    "\n",
    "# Teste com clÃ¡usula ÃšNICA (rÃ¡pido)\n",
    "print(\"\\nğŸ§ª TESTE 1: AnÃ¡lise de clÃ¡usula Ãºnica\")\n",
    "\n",
    "clausula_teste = \"Multa de 10% por atraso na entrega.\"\n",
    "resultado = pipeline.analisar_clausula_rapido(clausula_teste)\n",
    "\n",
    "print(f\"\\nğŸ“‹ RESULTADO:\")\n",
    "print(f\"   Texto: {resultado['texto']}\")\n",
    "print(f\"   Tipo: {resultado['tipo']}\")\n",
    "print(f\"   ConfianÃ§a: {resultado['confianca']:.1%}\")\n",
    "print(f\"   Tempo: {resultado['tempo_ms']:.1f}ms\")\n",
    "print(f\"   Tokens: {resultado['tokens']}\")\n",
    "\n",
    "# Teste com contrato PEQUENO\n",
    "print(\"\\nğŸ§ª TESTE 2: AnÃ¡lise de contrato pequeno\")\n",
    "\n",
    "contrato_pequeno_real = \"\"\"\n",
    "CONTRATO DE PRESTAÃ‡ÃƒO DE SERVIÃ‡OS\n",
    "\n",
    "CLÃUSULA 1 - OBJETO. O presente contrato tem por objeto a consultoria jurÃ­dica.\n",
    "\n",
    "CLÃUSULA 2 - PRAZO. VigÃªncia de 6 meses.\n",
    "\n",
    "CLÃUSULA 3 - VALOR. R$ 8.000,00 mensais.\n",
    "\n",
    "CLÃUSULA 4 - CONFIDENCIALIDADE. As partes mantÃªm sigilo.\n",
    "\"\"\"\n",
    "\n",
    "resultados = pipeline.analisar_contrato_rapido(contrato_pequeno_real)\n",
    "\n",
    "print(\"\\nâœ… PIPELINE FUNCIONANDO NA GPU 2GB!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "823cd328-0172-47e1-a77b-35a504bfa392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ğŸ§  GERENCIAMENTO DE MEMÃ“RIA PRÃTICO\n",
      "======================================================================\n",
      "\n",
      "ğŸ¯ COMANDOS ESSENCIAIS PARA SUA GPU 2GB:\n",
      "\n",
      "1. Antes de carregar\n",
      "   ğŸ’» torch.cuda.empty_cache()\n",
      "   ğŸ“ Limpa cache da GPU\n",
      "\n",
      "2. ApÃ³s usar modelo\n",
      "   ğŸ’» del modelo\n",
      "   ğŸ“ Remove referÃªncia\n",
      "\n",
      "3. ForÃ§ar coleta\n",
      "   ğŸ’» import gc; gc.collect()\n",
      "   ğŸ“ Libera memÃ³ria RAM\n",
      "\n",
      "4. Sempre usar\n",
      "   ğŸ’» with torch.no_grad():\n",
      "   ğŸ“ InferÃªncia sem gradientes\n",
      "\n",
      "5. PrecisÃ£o\n",
      "   ğŸ’» model.half()\n",
      "   ğŸ“ float16 (metade da memÃ³ria)\n",
      "\n",
      "ğŸ” DEMONSTRAÃ‡ÃƒO PRÃTICA DE MEMÃ“RIA:\n",
      "   MemÃ³ria atual: 0.23 GB\n",
      "   Com tensor: 0.24 GB\n",
      "   ApÃ³s deletar: 0.23 GB\n",
      "   âœ… Liberado: 0.004 GB\n"
     ]
    }
   ],
   "source": [
    "# TÃ‰CNICA ESSENCIAL 3 - Gerenciamento de MemÃ³ria PRÃTICO\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ§  GERENCIAMENTO DE MEMÃ“RIA PRÃTICO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nğŸ¯ COMANDOS ESSENCIAIS PARA SUA GPU 2GB:\")\n",
    "\n",
    "comandos_essenciais = [\n",
    "    (\"1. Antes de carregar\", \"torch.cuda.empty_cache()\", \"Limpa cache da GPU\"),\n",
    "    (\"2. ApÃ³s usar modelo\", \"del modelo\", \"Remove referÃªncia\"),\n",
    "    (\"3. ForÃ§ar coleta\", \"import gc; gc.collect()\", \"Libera memÃ³ria RAM\"),\n",
    "    (\"4. Sempre usar\", \"with torch.no_grad():\", \"InferÃªncia sem gradientes\"),\n",
    "    (\"5. PrecisÃ£o\", \"model.half()\", \"float16 (metade da memÃ³ria)\"),\n",
    "]\n",
    "\n",
    "for num, comando, desc in comandos_essenciais:\n",
    "    print(f\"\\n{num}\")\n",
    "    print(f\"   ğŸ’» {comando}\")\n",
    "    print(f\"   ğŸ“ {desc}\")\n",
    "\n",
    "# DemonstraÃ§Ã£o PRÃTICA\n",
    "print(\"\\nğŸ” DEMONSTRAÃ‡ÃƒO PRÃTICA DE MEMÃ“RIA:\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # Estado atual\n",
    "    memoria_inicial = torch.cuda.memory_allocated() / 1e9\n",
    "    print(f\"   MemÃ³ria atual: {memoria_inicial:.2f} GB\")\n",
    "    \n",
    "    # Criar e deletar tensor\n",
    "    tensor_teste = torch.randn(1000, 1000, device='cuda')  # ~4MB\n",
    "    memoria_com_tensor = torch.cuda.memory_allocated() / 1e9\n",
    "    print(f\"   Com tensor: {memoria_com_tensor:.2f} GB\")\n",
    "    \n",
    "    del tensor_teste\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    memoria_final = torch.cuda.memory_allocated() / 1e9\n",
    "    print(f\"   ApÃ³s deletar: {memoria_final:.2f} GB\")\n",
    "    print(f\"   âœ… Liberado: {memoria_com_tensor - memoria_final:.3f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43e94496-1bbd-4396-85b6-fe55e53b24ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ğŸš€ PRÃ“XIMOS PASSOS - FOCO NO PRÃTICO\n",
      "======================================================================\n",
      "\n",
      "âœ… VOCÃŠ JÃ TEM (do notebook leve):\n",
      "1. Pipeline jurÃ­dico funcional\n",
      "2. Chunking simples para contratos longos  \n",
      "3. AnÃ¡lise com BERT em portuguÃªs\n",
      "4. Gerenciamento de memÃ³ria bÃ¡sico\n",
      "\n",
      "ğŸ¯ PRÃ“XIMO: MÃ³dulo 04 - FINE-TUNING PRÃTICO\n",
      "\n",
      "CONTEÃšDO DO MÃ“DULO 04:\n",
      "1. Preparar SEUS documentos jurÃ­dicos como dataset\n",
      "2. Fine-tuning de BERT para SUAS classificaÃ§Ãµes\n",
      "3. AvaliaÃ§Ã£o do modelo fine-tuned\n",
      "4. ComparaÃ§Ã£o antes/depois do fine-tuning\n",
      "\n",
      "ğŸ“ ESTRUTURA SUGERIDA:\n",
      "04_Fine_Tuning_Pratico/\n",
      "â”œâ”€â”€ 01_preparar_dados.ipynb\n",
      "â”œâ”€â”€ 02_fine_tuning_bert.ipynb  \n",
      "â”œâ”€â”€ 03_avaliacao_modelo.ipynb\n",
      "â””â”€â”€ dados/\n",
      "    â”œâ”€â”€ contratos/\n",
      "    â”œâ”€â”€ classificacoes/\n",
      "    â””â”€â”€ modelo_fine_tuned/\n",
      "\n",
      "\n",
      "ğŸ” STATUS FINAL DA SUA GPU 2GB:\n",
      "   ğŸ’¾ MemÃ³ria usada: 0.23 GB\n",
      "   ğŸ¯ MemÃ³ria livre: 1.87 GB\n",
      "   âœ… Excelente! Pronto para fine-tuning.\n",
      "\n",
      "======================================================================\n",
      "ğŸ‰ NOTEBOOK 03 LEVE CONCLUÃDO EM MINUTOS!\n",
      "======================================================================\n",
      "\n",
      "ğŸ† RESUMO DO APRENDIZADO:\n",
      "1. âœ… Evitar downloads repetidos (cache)\n",
      "2. âœ… Pipeline jurÃ­dico funcional\n",
      "3. âœ… Chunking simples implementado\n",
      "4. âœ… Gerenciamento de memÃ³ria prÃ¡tico\n",
      "\n",
      "âš¡ PRÃ“XIMO: Fine-tuning do BERT com SEUS dados jurÃ­dicos!\n"
     ]
    }
   ],
   "source": [
    "# CÃ©lula 6: PRÃ“XIMOS PASSOS IMEDIATOS\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸš€ PRÃ“XIMOS PASSOS - FOCO NO PRÃTICO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "âœ… VOCÃŠ JÃ TEM (do notebook leve):\n",
    "1. Pipeline jurÃ­dico funcional\n",
    "2. Chunking simples para contratos longos  \n",
    "3. AnÃ¡lise com BERT em portuguÃªs\n",
    "4. Gerenciamento de memÃ³ria bÃ¡sico\n",
    "\n",
    "ğŸ¯ PRÃ“XIMO: MÃ³dulo 04 - FINE-TUNING PRÃTICO\n",
    "\n",
    "CONTEÃšDO DO MÃ“DULO 04:\n",
    "1. Preparar SEUS documentos jurÃ­dicos como dataset\n",
    "2. Fine-tuning de BERT para SUAS classificaÃ§Ãµes\n",
    "3. AvaliaÃ§Ã£o do modelo fine-tuned\n",
    "4. ComparaÃ§Ã£o antes/depois do fine-tuning\n",
    "\n",
    "ğŸ“ ESTRUTURA SUGERIDA:\n",
    "04_Fine_Tuning_Pratico/\n",
    "â”œâ”€â”€ 01_preparar_dados.ipynb\n",
    "â”œâ”€â”€ 02_fine_tuning_bert.ipynb  \n",
    "â”œâ”€â”€ 03_avaliacao_modelo.ipynb\n",
    "â””â”€â”€ dados/\n",
    "    â”œâ”€â”€ contratos/\n",
    "    â”œâ”€â”€ classificacoes/\n",
    "    â””â”€â”€ modelo_fine_tuned/\n",
    "\"\"\")\n",
    "\n",
    "# VerificaÃ§Ã£o final\n",
    "print(\"\\nğŸ” STATUS FINAL DA SUA GPU 2GB:\")\n",
    "if torch.cuda.is_available():\n",
    "    memoria_atual = torch.cuda.memory_allocated() / 1e9\n",
    "    print(f\"   ğŸ’¾ MemÃ³ria usada: {memoria_atual:.2f} GB\")\n",
    "    print(f\"   ğŸ¯ MemÃ³ria livre: {2.1 - memoria_atual:.2f} GB\")\n",
    "    \n",
    "    if memoria_atual < 0.5:\n",
    "        print(\"   âœ… Excelente! Pronto para fine-tuning.\")\n",
    "    else:\n",
    "        print(\"   âš ï¸  Considere limpar memÃ³ria antes do prÃ³ximo mÃ³dulo.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ‰ NOTEBOOK 03 LEVE CONCLUÃDO EM MINUTOS!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nğŸ† RESUMO DO APRENDIZADO:\")\n",
    "print(\"1. âœ… Evitar downloads repetidos (cache)\")\n",
    "print(\"2. âœ… Pipeline jurÃ­dico funcional\")\n",
    "print(\"3. âœ… Chunking simples implementado\")\n",
    "print(\"4. âœ… Gerenciamento de memÃ³ria prÃ¡tico\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4c9125-71e3-40e6-816e-ecfd58b24182",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Especialista IA",
   "language": "python",
   "name": "esp_ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
