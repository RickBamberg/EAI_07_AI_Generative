{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bfd4989-c0c7-4598-9764-1fc3a442d5b0",
   "metadata": {},
   "source": [
    "# ðŸš€ 01 - IntroduÃ§Ã£o PrÃ¡tica ao HuggingFace\n",
    "\n",
    "**Objetivo:** Em 30 minutos, fazer inferÃªncia com seu primeiro LLM!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e59c415-ed0c-4237-8209-f38ec561c283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ PyTorch: 2.3.1+cu118\n",
      "ðŸŽ® GPU disponÃ­vel: True\n",
      "ðŸ’» GPU: NVIDIA GeForce 930M\n",
      "ðŸ’¾ VRAM: 2.1 GB\n"
     ]
    }
   ],
   "source": [
    "# ImportaÃ§Ãµes bÃ¡sicas\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "print(f\"ðŸ”§ PyTorch: {torch.__version__}\")\n",
    "print(f\"ðŸŽ® GPU disponÃ­vel: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"ðŸ’» GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"ðŸ’¾ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a2a1ea9-0217-456a-b90e-1bc5e40d6ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¯ PRIMEIRO LLM EM 2 MINUTOS:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pcwin\\anaconda3\\envs\\esp_ai\\Lib\\site-packages\\huggingface_hub\\file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d60537be8334d0687fde20ae5b4c6e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dd5412092f04c67866bb14783515c83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bd8071efbce4fbf99056ff27a28fdc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6478dff11a924933991db4176e16a329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d11324f6aec3415390a8a87f2581537a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0030906ba8a4b21954c49da125a8024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f8340538ebb45f7902209d8dcf96346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Resultado: A inteligÃªncia artificial intelligence (AI), a project created to better understand the information needed to determine the needs of the user.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Primeiro pipeline (SIMPLES!)\n",
    "print(\"\\nðŸŽ¯ PRIMEIRO LLM EM 2 MINUTOS:\")\n",
    "\n",
    "# Modelo super leve para testar\n",
    "generator = pipeline('text-generation', model='distilgpt2')\n",
    "\n",
    "result = generator(\"A inteligÃªncia artificial\", max_length=30, num_return_sequences=1)\n",
    "print(\"âœ… Resultado:\", result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c428d9e8-6681-4514-b48e-f1d2bf6a06f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” TESTANDO MODELOS COMPATÃVEIS COM 2GB:\n",
      "\n",
      "ðŸ”§ Tentando: GPT-2 pequeno (82M) - RÃ¡pido\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Funcionou: No Direito brasileiro have always been popular with kids. They feature some of the classic brasileir...\n"
     ]
    }
   ],
   "source": [
    "# Testando diferentes modelos LEVES\n",
    "print(\"\\nðŸ” TESTANDO MODELOS COMPATÃVEIS COM 2GB:\")\n",
    "\n",
    "modelos_leves = [\n",
    "    ('distilgpt2', 'GPT-2 pequeno (82M) - RÃ¡pido'),\n",
    "    ('microsoft/phi-2', 'Phi-2 (2.7B) - Qualidade, precisa 4-bit'),\n",
    "    ('TinyLlama/TinyLlama-1.1B', 'TinyLlama (1.1B) - Equilibrado'),\n",
    "]\n",
    "\n",
    "for modelo, desc in modelos_leves[:1]:  # SÃ³ o primeiro por enquanto\n",
    "    try:\n",
    "        print(f\"\\nðŸ”§ Tentando: {desc}\")\n",
    "        generator = pipeline('text-generation', model=modelo, max_length=50)\n",
    "        result = generator(\"No Direito brasileiro\", num_return_sequences=1)\n",
    "        print(f\"âœ… Funcionou: {result[0]['generated_text'][:100]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Erro (provavelmente memÃ³ria): {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d605c35-1609-4a69-b544-980736aef2c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Especialista IA",
   "language": "python",
   "name": "esp_ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
