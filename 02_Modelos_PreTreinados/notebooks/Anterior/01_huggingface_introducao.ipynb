{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bfd4989-c0c7-4598-9764-1fc3a442d5b0",
   "metadata": {},
   "source": [
    "# üöÄ 01 - Introdu√ß√£o Pr√°tica ao HuggingFace\n",
    "## ‚ö° Otimizado para GPU 2GB | Foco em Portugu√™s e Jur√≠dico\n",
    "\n",
    "**Objetivo:** Primeiro contato pr√°tico com LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74711929-120a-4727-8632-788ae23ae9cd",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è IMPORTANTE: Limites da Sua GPU\n",
    "- **VRAM:** 2.1 GB \n",
    "- **Modelos que cabem:** ‚â§ 3B par√¢metros (com otimiza√ß√£o)\n",
    "- **Sequ√™ncia m√°xima:** ~512 tokens\n",
    "- **Estrat√©gia:** Usar modelos pequenos OU quantiza√ß√£o\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Objetivos Desta Sess√£o\n",
    "1. Carregar modelos em portugu√™s\n",
    "2. Aprender quantiza√ß√£o b√°sica\n",
    "3. Fazer primeira aplica√ß√£o jur√≠dica\n",
    "4. Identificar pr√≥ximos passos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e59c415-ed0c-4237-8209-f38ec561c283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üß™ DIAGN√ìSTICO DO SISTEMA\n",
      "======================================================================\n",
      "üîß PyTorch: 2.3.1+cu118\n",
      "üéÆ GPU dispon√≠vel: True\n",
      "üíª GPU: NVIDIA GeForce 930M\n",
      "üíæ VRAM: 2.1 GB\n",
      "\n",
      "‚ö†Ô∏è  ALERTA: GPU COM MEM√ìRIA LIMITADA\n",
      "‚úÖ Estrat√©gias que vamos usar:\n",
      "   1. Modelos pequenos (‚â§ 3B par√¢metros)\n",
      "   2. Quantiza√ß√£o 4-bit/8-bit\n",
      "   3. Sequ√™ncias curtas (‚â§ 512 tokens)\n",
      "   4. Batch size = 1\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Configura√ß√£o e diagn√≥stico\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üß™ DIAGN√ìSTICO DO SISTEMA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"üîß PyTorch: {torch.__version__}\")\n",
    "print(f\"üéÆ GPU dispon√≠vel: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    \n",
    "    print(f\"üíª GPU: {gpu_name}\")\n",
    "    print(f\"üíæ VRAM: {vram_gb:.1f} GB\")\n",
    "    \n",
    "    # Recomenda√ß√µes baseadas na GPU\n",
    "    if vram_gb < 3:\n",
    "        print(\"\\n‚ö†Ô∏è  ALERTA: GPU COM MEM√ìRIA LIMITADA\")\n",
    "        print(\"‚úÖ Estrat√©gias que vamos usar:\")\n",
    "        print(\"   1. Modelos pequenos (‚â§ 3B par√¢metros)\")\n",
    "        print(\"   2. Quantiza√ß√£o 4-bit/8-bit\")\n",
    "        print(\"   3. Sequ√™ncias curtas (‚â§ 512 tokens)\")\n",
    "        print(\"   4. Batch size = 1\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Sem GPU - Usando CPU (ser√° lento)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a2a1ea9-0217-456a-b90e-1bc5e40d6ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ DESAFIO: MODELOS EM INGL√äS VS PORTUGU√äS\n",
      "======================================================================\n",
      "\n",
      "1. Testando GPT-2 (ingl√™s):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt (PT): 'No Direito brasileiro, o contrato de trabalho'\n",
      "Resposta (EN): No Direito brasileiro, o contrato de trabalho y tambouso, o cambre, o il fasco de germano de de la spicca coma! Pare a una gente\n",
      "üëâ Problema: Modelo treinado em ingl√™s, responde em ingl√™s!\n"
     ]
    }
   ],
   "source": [
    "# O Problema - Modelos em Ingl√™s\n",
    "print(\"üéØ DESAFIO: MODELOS EM INGL√äS VS PORTUGU√äS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Teste com GPT-2 (ingl√™s)\n",
    "print(\"\\n1. Testando GPT-2 (ingl√™s):\")\n",
    "gpt2_generator = pipeline('text-generation', model='gpt2', max_length=50)\n",
    "\n",
    "prompt_pt = \"No Direito brasileiro, o contrato de trabalho\"\n",
    "result_en = gpt2_generator(prompt_pt, num_return_sequences=1)\n",
    "print(f\"Prompt (PT): '{prompt_pt}'\")\n",
    "print(f\"Resposta (EN): {result_en[0]['generated_text']}\")\n",
    "print(\"üëâ Problema: Modelo treinado em ingl√™s, responde em ingl√™s!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c428d9e8-6681-4514-b48e-f1d2bf6a06f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üáßüá∑ SOLU√á√ÉO: MODELOS PARA PORTUGU√äS\n",
      "======================================================================\n",
      "\n",
      "üéØ Op√ß√µes para portugu√™s (do mais leve ao mais pesado):\n",
      "\n",
      "1. pierreguillou/gpt2-small-portuguese\n",
      "   üìù GPT-2 pequeno em portugu√™s\n",
      "   üìä 124M par√¢metros\n",
      "   üí° ‚úÖ PERFEITO para 2GB\n",
      "\n",
      "2. pierreguillou/gpt2-medium-portuguese\n",
      "   üìù GPT-2 m√©dio em portugu√™s\n",
      "   üìä 355M par√¢metros\n",
      "   üí° ‚úÖ CABE na 2GB\n",
      "\n",
      "3. neuralmind/bert-base-portuguese-cased\n",
      "   üìù BERT para portugu√™s (encoder)\n",
      "   üìä 110M par√¢metros\n",
      "   üí° ‚úÖ CABE, mas √© BERT (n√£o gera texto)\n",
      "\n",
      "4. microsoft/phi-2\n",
      "   üìù Phi-2 (2.7B) - Multil√≠ngue\n",
      "   üìä 2.7B par√¢metros\n",
      "   üí° ‚ö†Ô∏è  PRECISA de quantiza√ß√£o 4-bit\n"
     ]
    }
   ],
   "source": [
    "# Solu√ß√£o - Modelos para Portugu√™s\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üáßüá∑ SOLU√á√ÉO: MODELOS PARA PORTUGU√äS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüéØ Op√ß√µes para portugu√™s (do mais leve ao mais pesado):\")\n",
    "\n",
    "modelos_pt = [\n",
    "    {\n",
    "        \"nome\": \"pierreguillou/gpt2-small-portuguese\",\n",
    "        \"desc\": \"GPT-2 pequeno em portugu√™s\",\n",
    "        \"tamanho\": \"124M par√¢metros\",\n",
    "        \"recomendacao\": \"‚úÖ PERFEITO para 2GB\"\n",
    "    },\n",
    "    {\n",
    "        \"nome\": \"pierreguillou/gpt2-medium-portuguese\", \n",
    "        \"desc\": \"GPT-2 m√©dio em portugu√™s\",\n",
    "        \"tamanho\": \"355M par√¢metros\",\n",
    "        \"recomendacao\": \"‚úÖ CABE na 2GB\"\n",
    "    },\n",
    "    {\n",
    "        \"nome\": \"neuralmind/bert-base-portuguese-cased\",\n",
    "        \"desc\": \"BERT para portugu√™s (encoder)\",\n",
    "        \"tamanho\": \"110M par√¢metros\", \n",
    "        \"recomendacao\": \"‚úÖ CABE, mas √© BERT (n√£o gera texto)\"\n",
    "    },\n",
    "    {\n",
    "        \"nome\": \"microsoft/phi-2\",\n",
    "        \"desc\": \"Phi-2 (2.7B) - Multil√≠ngue\",\n",
    "        \"tamanho\": \"2.7B par√¢metros\",\n",
    "        \"recomendacao\": \"‚ö†Ô∏è  PRECISA de quantiza√ß√£o 4-bit\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, modelo in enumerate(modelos_pt, 1):\n",
    "    print(f\"\\n{i}. {modelo['nome']}\")\n",
    "    print(f\"   üìù {modelo['desc']}\")\n",
    "    print(f\"   üìä {modelo['tamanho']}\")\n",
    "    print(f\"   üí° {modelo['recomendacao']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d605c35-1609-4a69-b544-980736aef2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üöÄ TESTANDO: GPT-2 Small Portugu√™s\n",
      "======================================================================\n",
      "\n",
      "üîß Carregando GPT-2 Small Portuguese...\n",
      "‚úÖ Modelo carregado com sucesso!\n",
      "\n",
      "üìù Prompt: 'No Direito brasileiro,'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=50) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=50) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Resposta: No Direito brasileiro, por ele sendo admitido nos cursos de Direito Internacional e na Universidad de Iju√≠.\n",
      "\n",
      "Foi professor catedr√°tico da Faculdade de Direito da Universidade Federal do Rio de Janeiro (UFRJ); de 1991 a 1995 foi coordenador do Departamento de Direito Internacional da Universidad Nacional\n",
      "\n",
      "üìù Prompt: 'O contrato de trabalho deve'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=50) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Resposta: O contrato de trabalho deve a ser entregue a uma pessoa contratada de forma definitiva em primeiro lugar, com prazo de um m√™s ou mais, em casa dos mesmos, at√© o final do per√≠odo, podendo tamb√©m ocorrer em um outro local onde n√£o o mesmo esteja em actividade.\n",
      "\n",
      "\n",
      "\n",
      "üìù Prompt: 'A Constitui√ß√£o Federal estabelece que'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=50) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Resposta: A Constitui√ß√£o Federal estabelece que:\n",
      "\n",
      "Para cumprir a preceito fundamental do Estado, as leis emanam dos particulares como forma de manifesta√ß√£o e por eles tamb√©m podem ser aplicadas nos diversos n√≠veis de governo o qual por suas decis√µes, incluindo controle das atividades governamentais, controle social que afeta\n",
      "\n",
      "üìù Prompt: 'O artigo 5¬∫ da CLT determina'\n",
      "   Resposta: O artigo 5¬∫ da CLT determina o princ√≠pio da igualdade entre os habitantes dos territ√≥rios de ambos os pa√≠ses signat√°rios. Segundo o artigo 6¬∫ da CLT, a lei brasileira determina o direito de ambos os pa√≠ses signat√°rios a manter suas fronteiras e tamb√©m a igualdade de direitos aduaneiros e trabalhistas\n"
     ]
    }
   ],
   "source": [
    "# Primeiro Modelo em Portugu√™s (GPT-2 Small PT)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üöÄ TESTANDO: GPT-2 Small Portugu√™s\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    print(\"\\nüîß Carregando GPT-2 Small Portuguese...\")\n",
    "    \n",
    "    # Pipeline simples\n",
    "    generator_pt = pipeline(\n",
    "        'text-generation', \n",
    "        model='pierreguillou/gpt2-small-portuguese',\n",
    "        max_length=100,\n",
    "        device=0 if torch.cuda.is_available() else -1\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Modelo carregado com sucesso!\")\n",
    "    \n",
    "    # Testes\n",
    "    testes = [\n",
    "        \"No Direito brasileiro,\",\n",
    "        \"O contrato de trabalho deve\",\n",
    "        \"A Constitui√ß√£o Federal estabelece que\",\n",
    "        \"O artigo 5¬∫ da CLT determina\"\n",
    "    ]\n",
    "    \n",
    "    for prompt in testes:\n",
    "        print(f\"\\nüìù Prompt: '{prompt}'\")\n",
    "        result = generator_pt(prompt, num_return_sequences=1, max_new_tokens=50)\n",
    "        print(f\"   Resposta: {result[0]['generated_text']}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro ao carregar modelo: {e}\")\n",
    "    print(\"\\nüí° Alternativa: Vamos tentar BERT para an√°lise (n√£o gera√ß√£o):\")\n",
    "    \n",
    "    # Fallback para BERT\n",
    "    from transformers import pipeline as pl\n",
    "    classifier = pl(\"text-classification\", model=\"neuralmind/bert-base-portuguese-cased\")\n",
    "    print(\"‚úÖ BERT carregado para classifica√ß√£o\")\n",
    "    print(\"   Teste: 'Este contrato √© v√°lido'\")\n",
    "    result = classifier(\"Este contrato √© v√°lido\")\n",
    "    print(f\"   Resultado: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2fda605-d120-433a-bcd2-e274d0c43e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "‚ö° QUANTIZA√á√ÉO: Fazendo modelos grandes caberem na 2GB\n",
      "======================================================================\n",
      "\n",
      "O que √© quantiza√ß√£o?\n",
      "‚Ä¢ Converter pesos de 32-bit ‚Üí 8-bit ou 4-bit\n",
      "‚Ä¢ Reduz mem√≥ria em 75% (8-bit) ou 87.5% (4-bit)\n",
      "‚Ä¢ Pouca perda de qualidade para LLMs\n",
      "\n",
      "PARA SUA GPU 2GB:\n",
      "‚Ä¢ Sem quantiza√ß√£o: apenas modelos ‚â§ 500M\n",
      "‚Ä¢ Com 8-bit: modelos ‚â§ 2B  \n",
      "‚Ä¢ Com 4-bit: modelos ‚â§ 3-4B\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Quantiza√ß√£o B√°sica (SALVA-VIDA para 2GB)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚ö° QUANTIZA√á√ÉO: Fazendo modelos grandes caberem na 2GB\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "O que √© quantiza√ß√£o?\n",
    "‚Ä¢ Converter pesos de 32-bit ‚Üí 8-bit ou 4-bit\n",
    "‚Ä¢ Reduz mem√≥ria em 75% (8-bit) ou 87.5% (4-bit)\n",
    "‚Ä¢ Pouca perda de qualidade para LLMs\n",
    "\n",
    "PARA SUA GPU 2GB:\n",
    "‚Ä¢ Sem quantiza√ß√£o: apenas modelos ‚â§ 500M\n",
    "‚Ä¢ Com 8-bit: modelos ‚â§ 2B  \n",
    "‚Ä¢ Com 4-bit: modelos ‚â§ 3-4B\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2921e315-e39f-4da1-9501-2e02a3b2f5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üß™ EXPERIMENTO: Phi-2 com 4-bit na GPU 2GB\n",
      "======================================================================\n",
      "\n",
      "üîß Tentando carregar Phi-2 com 4-bit...\n",
      "üì• Baixando modelo (pode demorar alguns minutos)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8774d4695e449e38c973c7b192c04b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a88081003ff4374b75f2ba295fab810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f8d72253c474b1eafc791d9b88aa7b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc00d1beb9ce4287a9d2be6e4f67f739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://huggingface.co/microsoft/phi-2/resolve/main/model-00001-of-00002.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /microsoft/phi-2/resolve/main/model-00001-of-00002.safetensors (Caused by NameResolutionError(\"HTTPSConnection(host=\\'huggingface.co\\', port=443): Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 3b5c8883-7947-4118-8294-fc134917f9c7)')' thrown while requesting GET https://huggingface.co/microsoft/phi-2/resolve/main/model-00001-of-00002.safetensors\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcedaacd87a6435b8421e4bc22e293b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:  45%|####5     | 2.25G/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7afc3b4de3342f399ed57ca8dfd2f6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/564M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Erro: \n",
      "                    Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the\n",
      "                    quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules\n",
      "                    in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to\n",
      "                    `from_pretrained`. Check\n",
      "                    https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n",
      "                    for more details.\n",
      "                    \n",
      "\n",
      "üí° Poss√≠veis solu√ß√µes:\n",
      "1. Instalar bitsandbytes: pip install bitsandbytes\n",
      "2. Usar vers√£o mais recente do transformers\n",
      "3. Tentar modelo menor primeiro\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Instalar bitsandbytes agora? (s/n):  s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instalando bitsandbytes...\n"
     ]
    }
   ],
   "source": [
    "# Primeiro modelo com quantiza√ß√£o (Phi-2 4-bit)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üß™ EXPERIMENTO: Phi-2 com 4-bit na GPU 2GB\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    print(\"\\nüîß Tentando carregar Phi-2 com 4-bit...\")\n",
    "    \n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "    import torch\n",
    "    \n",
    "    # Configura√ß√£o 4-bit\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True\n",
    "    )\n",
    "    \n",
    "    # Carregar modelo com quantiza√ß√£o\n",
    "    model_name = \"microsoft/phi-2\"\n",
    "    \n",
    "    print(\"üì• Baixando modelo (pode demorar alguns minutos)...\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Phi-2 carregado com 4-bit!\")\n",
    "    print(f\"   Mem√≥ria usada: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "    \n",
    "    # Teste r√°pido\n",
    "    prompt = \"Explique o que √© um contrato de trabalho:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "    \n",
    "    resposta = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"\\nüìù Prompt: {prompt}\")\n",
    "    print(f\"ü§ñ Resposta: {resposta}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro: {e}\")\n",
    "    print(\"\\nüí° Poss√≠veis solu√ß√µes:\")\n",
    "    print(\"1. Instalar bitsandbytes: pip install bitsandbytes\")\n",
    "    print(\"2. Usar vers√£o mais recente do transformers\")\n",
    "    print(\"3. Tentar modelo menor primeiro\")\n",
    "    \n",
    "    # Instala√ß√£o r√°pida se necess√°rio\n",
    "    import sys\n",
    "    import subprocess\n",
    "    \n",
    "    install = input(\"\\nüì¶ Instalar bitsandbytes agora? (s/n): \")\n",
    "    if install.lower() == 's':\n",
    "        print(\"Instalando bitsandbytes...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"bitsandbytes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce8bd43b-4b36-42dd-aad1-8c3fadd52956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "‚öñÔ∏è APLICA√á√ÉO PR√ÅTICA: An√°lise de Cl√°usula Contratual\n",
      "======================================================================\n",
      "\n",
      "Vamos criar uma ferramenta simples para an√°lise jur√≠dica.\n",
      "\n",
      "Funcionalidade:\n",
      "1. Extrair termos importantes de cl√°usulas\n",
      "2. Classificar tipo de cl√°usula\n",
      "3. Sugerir pontos de aten√ß√£o\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=150) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modelo gerador carregado para an√°lise jur√≠dica\n",
      "\n",
      "üìÑ Cl√°usula para an√°lise:\n",
      "\n",
      "    CL√ÅUSULA QUARTA - DA MULTA. Em caso de descumprimento das obriga√ß√µes aqui pactuadas, \n",
      "    o inadimplente pagar√° multa correspondente a 10% do valor do contrato, \n",
      "    sem preju√≠zo das perdas e danos.\n",
      "    \n",
      "\n",
      "üîç An√°lise da IA:\n",
      "Analise esta cl√°usula contratual e identifique:\n",
      "1. Tipo de cl√°usula\n",
      "2. Elementos principais\n",
      "3. Poss√≠veis problemas\n",
      "\n",
      "Cl√°usula: \n",
      "    CL√ÅUSULA QUARTA - DA MULTA. Em caso de descumprimento das obriga√ß√µes aqui pactuadas, \n",
      "    o inadimplente pagar√° multa correspondente a 10% do valor do contrato, \n",
      "    sem preju√≠zo das perdas e danos.\n",
      "    \n",
      "\n",
      "An√°lise: \n",
      "1. Tipo de cl√°usula contratual \n",
      "\n",
      "2. Elementos principais\n",
      "\n",
      "3. Poss√≠veis problemas\n",
      "\n",
      "Cl√°usula: \n",
      "1. ¬†¬†¬†¬†¬†¬†¬† ¬† IN DIANDA√á√ÉO QUARTA - DA MULTA. Em caso de descumprimento das obriga√ß√µes aqui pactuadas, \n",
      "   os inadimplentes pagar√° multa correspondente a 10% do valor do contrato, \n",
      "   sem preju√≠zo das perdas e danos.\n",
      " ¬† ¬† ¬† ¬† ¬† ¬† ¬† \n",
      "An√°lise: \n",
      "1. ¬†¬†¬†¬†¬†¬†¬† \n",
      "2. ARTENGIZA√á√ÉO DA NORTE. Em caso de descumprimento das obriga√ß√µes aqui pactuadas, \n",
      "   os inadimplentes pagar√° multa\n"
     ]
    }
   ],
   "source": [
    "# Aplica√ß√£o Jur√≠dica Pr√°tica\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚öñÔ∏è APLICA√á√ÉO PR√ÅTICA: An√°lise de Cl√°usula Contratual\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "Vamos criar uma ferramenta simples para an√°lise jur√≠dica.\n",
    "\n",
    "Funcionalidade:\n",
    "1. Extrair termos importantes de cl√°usulas\n",
    "2. Classificar tipo de cl√°usula\n",
    "3. Sugerir pontos de aten√ß√£o\n",
    "\"\"\")\n",
    "\n",
    "# Carregar modelo para portugu√™s (usando o que funcionou)\n",
    "try:\n",
    "    # Tenta carregar GPT-2 PT, se n√£o, usa BERT\n",
    "    try:\n",
    "        generator = pipeline(\n",
    "            'text-generation',\n",
    "            model='pierreguillou/gpt2-small-portuguese',\n",
    "            max_length=200,\n",
    "            device=0\n",
    "        )\n",
    "        modelo_tipo = \"gerador\"\n",
    "    except:\n",
    "        from transformers import pipeline as pl\n",
    "        classifier = pl(\"text-classification\", model=\"neuralmind/bert-base-portuguese-cased\")\n",
    "        modelo_tipo = \"classificador\"\n",
    "    \n",
    "    print(f\"‚úÖ Modelo {modelo_tipo} carregado para an√°lise jur√≠dica\")\n",
    "    \n",
    "    # Exemplo de cl√°usula\n",
    "    clausula = \"\"\"\n",
    "    CL√ÅUSULA QUARTA - DA MULTA. Em caso de descumprimento das obriga√ß√µes aqui pactuadas, \n",
    "    o inadimplente pagar√° multa correspondente a 10% do valor do contrato, \n",
    "    sem preju√≠zo das perdas e danos.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nüìÑ Cl√°usula para an√°lise:\")\n",
    "    print(clausula)\n",
    "    \n",
    "    if modelo_tipo == \"gerador\":\n",
    "        prompt = f\"Analise esta cl√°usula contratual e identifique:\\n1. Tipo de cl√°usula\\n2. Elementos principais\\n3. Poss√≠veis problemas\\n\\nCl√°usula: {clausula}\\n\\nAn√°lise:\"\n",
    "        \n",
    "        resposta = generator(prompt, max_new_tokens=150, num_return_sequences=1)\n",
    "        print(f\"\\nüîç An√°lise da IA:\")\n",
    "        print(resposta[0]['generated_text'])\n",
    "    \n",
    "    else:\n",
    "        # An√°lise com classificador\n",
    "        analises = [\n",
    "            \"Esta cl√°usula estabelece penalidade por descumprimento\",\n",
    "            \"O valor da multa √© de 10% do contrato\",\n",
    "            \"H√° previs√£o de perdas e danos al√©m da multa\"\n",
    "        ]\n",
    "        \n",
    "        print(\"\\nüîç Classifica√ß√µes:\")\n",
    "        for texto in analises:\n",
    "            resultado = classifier(texto)\n",
    "            print(f\"   '{texto}' ‚Üí {resultado[0]['label']} ({resultado[0]['score']:.2%})\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro na an√°lise: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1ee81dd-768c-4a4e-8ef1-63510444654e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üìà RESUMO DO APRENDIZADO E PR√ìXIMOS PASSOS\n",
      "======================================================================\n",
      "\n",
      "‚úÖ O QUE FUNCIONOU NA SUA GPU 2GB:\n",
      "1. DistilGPT-2 (ingl√™s) - Funciona, mas n√£o ideal para PT\n",
      "2. GPT-2 Small Portuguese - √ìtimo! Cabe e gera em portugu√™s\n",
      "3. BERTimbau - Para an√°lise/classifica√ß√£o\n",
      "4. Phi-2 com 4-bit - Potencial, precisa de bitsandbytes\n",
      "\n",
      "‚ö° ESTRAT√âGIAS EFETIVAS PARA 2GB:\n",
      "‚Ä¢ Modelos ‚â§ 500M par√¢metros (sem quantiza√ß√£o)\n",
      "‚Ä¢ Modelos ‚â§ 2B par√¢metros (com 8-bit)\n",
      "‚Ä¢ Modelos ‚â§ 3-4B par√¢metros (com 4-bit)\n",
      "‚Ä¢ Sequ√™ncias ‚â§ 512 tokens\n",
      "‚Ä¢ Batch size = 1\n",
      "\n",
      "\n",
      "üîç STATUS ATUAL DA GPU:\n",
      "   Mem√≥ria alocada: 0.80 GB\n",
      "   Mem√≥ria reservada: 0.86 GB\n",
      "   Dispon√≠vel: 1.30 GB\n",
      "   ‚úÖ Mem√≥ria OK para pr√≥ximos experimentos.\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Pr√≥ximos Passos e Resumo\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìà RESUMO DO APRENDIZADO E PR√ìXIMOS PASSOS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "‚úÖ O QUE FUNCIONOU NA SUA GPU 2GB:\n",
    "1. DistilGPT-2 (ingl√™s) - Funciona, mas n√£o ideal para PT\n",
    "2. GPT-2 Small Portuguese - √ìtimo! Cabe e gera em portugu√™s\n",
    "3. BERTimbau - Para an√°lise/classifica√ß√£o\n",
    "4. Phi-2 com 4-bit - Potencial, precisa de bitsandbytes\n",
    "\n",
    "‚ö° ESTRAT√âGIAS EFETIVAS PARA 2GB:\n",
    "‚Ä¢ Modelos ‚â§ 500M par√¢metros (sem quantiza√ß√£o)\n",
    "‚Ä¢ Modelos ‚â§ 2B par√¢metros (com 8-bit)\n",
    "‚Ä¢ Modelos ‚â§ 3-4B par√¢metros (com 4-bit)\n",
    "‚Ä¢ Sequ√™ncias ‚â§ 512 tokens\n",
    "‚Ä¢ Batch size = 1\n",
    "\"\"\")\n",
    "\n",
    "# Verifica√ß√£o final\n",
    "print(\"\\nüîç STATUS ATUAL DA GPU:\")\n",
    "if torch.cuda.is_available():\n",
    "    allocated = torch.cuda.memory_allocated() / 1e9\n",
    "    reserved = torch.cuda.memory_reserved() / 1e9\n",
    "    \n",
    "    print(f\"   Mem√≥ria alocada: {allocated:.2f} GB\")\n",
    "    print(f\"   Mem√≥ria reservada: {reserved:.2f} GB\")\n",
    "    print(f\"   Dispon√≠vel: {(2.1 - allocated):.2f} GB\")\n",
    "    \n",
    "    if allocated > 1.5:\n",
    "        print(\"   ‚ö†Ô∏è  Mem√≥ria quase cheia! Use quantiza√ß√£o.\")\n",
    "    else:\n",
    "        print(\"   ‚úÖ Mem√≥ria OK para pr√≥ximos experimentos.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec1f8fe-5e9a-4b24-86ba-d84bef3e9f1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Especialista IA",
   "language": "python",
   "name": "esp_ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
