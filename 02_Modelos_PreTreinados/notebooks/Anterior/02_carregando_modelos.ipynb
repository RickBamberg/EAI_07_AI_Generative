{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81ffc78d-9b09-4a0f-b3ff-4b2315ffd0d2",
   "metadata": {},
   "source": [
    "# ğŸš€ 02 - Carregando Modelos com PropÃ³sito\n",
    "## âš–ï¸ Do Gerador de Texto ao Assistente JurÃ­dico\n",
    "\n",
    "**Objetivo:** Aprender a carregar e usar modelos especializados para aplicaÃ§Ãµes jurÃ­dicas reais.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‹ LiÃ§Ãµes do Notebook Anterior\n",
    "1. âœ… **GPT-2 Small Portuguese** funciona na sua GPU (2.1 GB VRAM)\n",
    "2. âŒ **Phi-2 (2.7B)** Ã© grande demais mesmo com 4-bit\n",
    "3. âš ï¸  Modelos base \"alucinam\" fatos jurÃ­dicos\n",
    "4. ğŸ’¡ **SoluÃ§Ã£o:** Usar modelos especializados + tÃ©cnicas de _prompt_\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Objetivos Deste Notebook\n",
    "1. Carregar diferentes tipos de modelos (generativo vs encoder)\n",
    "2. Usar modelos jurÃ­dicos especializados em portuguÃªs\n",
    "3. Aplicar _device_map=\"auto\"_ para gerenciamento inteligente de memÃ³ria\n",
    "4. Criar um pipeline simples de anÃ¡lise contratual\n",
    "\n",
    "---\n",
    "\n",
    "## âš¡ EstratÃ©gia para GPU 2GB\n",
    "- **Modelos â‰¤ 500M parÃ¢metros:** Carregamento direto\n",
    "- **Modelos â‰¤ 2B parÃ¢metros:** Necessitam otimizaÃ§Ã£o\n",
    "- **device_map=\"auto\":** Distribui camadas automaticamente entre GPU/CPU\n",
    "- **SequÃªncias curtas:** â‰¤ 512 tokens para anÃ¡lise jurÃ­dica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ba3ff76-2de0-49a0-8746-e84bfb92bbdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ğŸ§® DIAGNÃ“STICO INICIAL DO SISTEMA\n",
      "======================================================================\n",
      "ğŸ”§ PyTorch: 2.3.1+cu118\n",
      "ğŸ® GPU disponÃ­vel: True\n",
      "ğŸ’» GPU: NVIDIA GeForce 930M\n",
      "ğŸ’¾ VRAM Total: 2.1 GB\n",
      "ğŸ’¾ VRAM Alocada: 0.00 GB\n",
      "ğŸ’¾ VRAM Livre: 2.15 GB\n",
      "âœ… Ã“timo! Pode carregar modelos atÃ© ~500M parÃ¢metros\n",
      "\n",
      "ğŸ§  RAM Total: 10.6 GB\n",
      "ğŸ§  RAM DisponÃ­vel: 4.6 GB\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ConfiguraÃ§Ã£o e DiagnÃ³stico de MemÃ³ria\n",
    "import torch\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoModelForSequenceClassification\n",
    ")\n",
    "import gc\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ§® DIAGNÃ“STICO INICIAL DO SISTEMA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# InformaÃ§Ãµes da GPU\n",
    "print(f\"ğŸ”§ PyTorch: {torch.__version__}\")\n",
    "print(f\"ğŸ® GPU disponÃ­vel: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    vram_total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    vram_alocada = torch.cuda.memory_allocated() / 1e9\n",
    "    vram_livre = vram_total - vram_alocada\n",
    "    \n",
    "    print(f\"ğŸ’» GPU: {gpu_name}\")\n",
    "    print(f\"ğŸ’¾ VRAM Total: {vram_total:.1f} GB\")\n",
    "    print(f\"ğŸ’¾ VRAM Alocada: {vram_alocada:.2f} GB\")\n",
    "    print(f\"ğŸ’¾ VRAM Livre: {vram_livre:.2f} GB\")\n",
    "    \n",
    "    # RecomendaÃ§Ãµes baseadas na memÃ³ria livre\n",
    "    if vram_livre > 1.5:\n",
    "        print(\"âœ… Ã“timo! Pode carregar modelos atÃ© ~500M parÃ¢metros\")\n",
    "    elif vram_livre > 0.8:\n",
    "        print(\"âš ï¸  Cuidado! Limite-se a modelos â‰¤ 300M parÃ¢metros\")\n",
    "    else:\n",
    "        print(\"âŒ MemÃ³ria muito baixa! Libere memÃ³ria antes de continuar\")\n",
    "\n",
    "# InformaÃ§Ãµes da RAM\n",
    "ram = psutil.virtual_memory()\n",
    "print(f\"\\nğŸ§  RAM Total: {ram.total / 1e9:.1f} GB\")\n",
    "print(f\"ğŸ§  RAM DisponÃ­vel: {ram.available / 1e9:.1f} GB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e837404-bbdd-42ef-ac70-d0fe70cbdd94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤” ENTENDENDO OS TIPOS DE MODELOS\n",
      "======================================================================\n",
      "\n",
      "ğŸ¯ RESUMO PARA SUA APLICAÃ‡ÃƒO JURÃDICA:\n",
      "1. Para GERAR texto (petiÃ§Ãµes, clÃ¡usulas): Use modelos generativos\n",
      "2. Para ANALISAR texto (contratos, leis): Use modelos encoder (BERT-like)\n",
      "3. Para RESUMIR/REFORMULAR: Use encoder-decoder (se couber na memÃ³ria)\n",
      "\n",
      "ğŸ“‹ COMPARAÃ‡ÃƒO PRÃTICA:\n",
      "\n",
      "1. Modelos Generativos (Decoder-only)\n",
      "   ğŸ“ Exemplo: GPT-2, GPT-3, LLaMA, Phi-2\n",
      "   ğŸ¯ Uso principal: RedaÃ§Ã£o, conclusÃ£o, criatividade\n",
      "   ğŸ’» Para sua GPU: âœ… GPT-2 Small PT funciona bem\n",
      "\n",
      "2. Modelos Encoder (BERT-like)\n",
      "   ğŸ“ Exemplo: BERT, RoBERTa, Legal-BERT\n",
      "   ğŸ¯ Uso principal: AnÃ¡lise, busca, NER, Q&A\n",
      "   ğŸ’» Para sua GPU: âœ… BERTimbau e variantes cabem\n",
      "\n",
      "3. Modelos Encoder-Decoder\n",
      "   ğŸ“ Exemplo: T5, BART\n",
      "   ğŸ¯ Uso principal: TraduÃ§Ã£o, sumarizaÃ§Ã£o, reformulaÃ§Ã£o\n",
      "   ğŸ’» Para sua GPU: âš ï¸  T5-small pode caber, mas testar\n"
     ]
    }
   ],
   "source": [
    "# Tipos Fundamentais de Modelos - Teoria PrÃ¡tica\n",
    "print(\"ğŸ¤” ENTENDENDO OS TIPOS DE MODELOS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "tipos_modelos = [\n",
    "    {\n",
    "        \"tipo\": \"Modelos Generativos (Decoder-only)\",\n",
    "        \"exemplo\": \"GPT-2, GPT-3, LLaMA, Phi-2\",\n",
    "        \"funÃ§Ã£o\": \"Gerar texto sequencialmente\",\n",
    "        \"uso\": \"RedaÃ§Ã£o, conclusÃ£o, criatividade\",\n",
    "        \"carregamento\": \"AutoModelForCausalLM\",\n",
    "        \"para_sua_gpu\": \"âœ… GPT-2 Small PT funciona bem\"\n",
    "    },\n",
    "    {\n",
    "        \"tipo\": \"Modelos Encoder (BERT-like)\",\n",
    "        \"exemplo\": \"BERT, RoBERTa, Legal-BERT\",\n",
    "        \"funÃ§Ã£o\": \"CompreensÃ£o/classificaÃ§Ã£o de texto\",\n",
    "        \"uso\": \"AnÃ¡lise, busca, NER, Q&A\",\n",
    "        \"carregamento\": \"AutoModelForMaskedLM ou AutoModelForSequenceClassification\",\n",
    "        \"para_sua_gpu\": \"âœ… BERTimbau e variantes cabem\"\n",
    "    },\n",
    "    {\n",
    "        \"tipo\": \"Modelos Encoder-Decoder\",\n",
    "        \"exemplo\": \"T5, BART\",\n",
    "        \"funÃ§Ã£o\": \"TransformaÃ§Ã£o de texto\",\n",
    "        \"uso\": \"TraduÃ§Ã£o, sumarizaÃ§Ã£o, reformulaÃ§Ã£o\",\n",
    "        \"carregamento\": \"AutoModelForSeq2SeqLM\",\n",
    "        \"para_sua_gpu\": \"âš ï¸  T5-small pode caber, mas testar\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"\\nğŸ¯ RESUMO PARA SUA APLICAÃ‡ÃƒO JURÃDICA:\")\n",
    "print(\"1. Para GERAR texto (petiÃ§Ãµes, clÃ¡usulas): Use modelos generativos\")\n",
    "print(\"2. Para ANALISAR texto (contratos, leis): Use modelos encoder (BERT-like)\")\n",
    "print(\"3. Para RESUMIR/REFORMULAR: Use encoder-decoder (se couber na memÃ³ria)\")\n",
    "\n",
    "print(\"\\nğŸ“‹ COMPARAÃ‡ÃƒO PRÃTICA:\")\n",
    "for i, tipo in enumerate(tipos_modelos, 1):\n",
    "    print(f\"\\n{i}. {tipo['tipo']}\")\n",
    "    print(f\"   ğŸ“ Exemplo: {tipo['exemplo']}\")\n",
    "    print(f\"   ğŸ¯ Uso principal: {tipo['uso']}\")\n",
    "    print(f\"   ğŸ’» Para sua GPU: {tipo['para_sua_gpu']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "faffe90d-f6c1-4dca-8eee-3ffc386c0769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "âš¡ DEVICE_MAP='AUTO': Seu Aliado para 2GB\n",
      "======================================================================\n",
      "\n",
      "O que Ã© device_map=\"auto\"?\n",
      "â€¢ Distribui automaticamente as camadas do modelo entre GPU, CPU e atÃ© disco\n",
      "â€¢ Otimiza o uso de memÃ³ria baseado no hardware disponÃ­vel\n",
      "â€¢ Para sua GPU 2GB: MantÃ©m camadas crÃ­ticas na GPU, offloada o resto\n",
      "\n",
      "COMO FUNCIONA NA PRÃTICA:\n",
      "1. Camadas mais usadas (atenÃ§Ã£o) â†’ GPU\n",
      "2. Camadas menos usadas (algumas feed-forward) â†’ CPU  \n",
      "3. Se necessÃ¡rio, algumas camadas â†’ Disco (mais lento)\n",
      "\n",
      "VANTAGENS PARA VOCÃŠ:\n",
      "â€¢ Carrega modelos maiores que nÃ£o caberiam totalmente na GPU\n",
      "â€¢ Balanceamento automÃ¡tico performance/memÃ³ria\n",
      "â€¢ ConfiguraÃ§Ã£o zero - sÃ³ passar o parÃ¢metro!\n",
      "\n",
      "\n",
      "ğŸ¯ EXEMPLO DE DISTRIBUIÃ‡ÃƒO PARA UM BERT-BASE (110M):\n",
      "   â€¢ GPU (2.1 GB): 70% das camadas (atenÃ§Ã£o + algumas FFN)\n",
      "   â€¢ CPU (RAM): 25% das camadas (FFN menos crÃ­ticas)\n",
      "   â€¢ Disco (opcional): 5% (se RAM tambÃ©m estiver limitada)\n"
     ]
    }
   ],
   "source": [
    "# device_map=\"auto\" - O Superpoder para GPU Limitada\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âš¡ DEVICE_MAP='AUTO': Seu Aliado para 2GB\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "O que Ã© device_map=\"auto\"?\n",
    "â€¢ Distribui automaticamente as camadas do modelo entre GPU, CPU e atÃ© disco\n",
    "â€¢ Otimiza o uso de memÃ³ria baseado no hardware disponÃ­vel\n",
    "â€¢ Para sua GPU 2GB: MantÃ©m camadas crÃ­ticas na GPU, offloada o resto\n",
    "\n",
    "COMO FUNCIONA NA PRÃTICA:\n",
    "1. Camadas mais usadas (atenÃ§Ã£o) â†’ GPU\n",
    "2. Camadas menos usadas (algumas feed-forward) â†’ CPU  \n",
    "3. Se necessÃ¡rio, algumas camadas â†’ Disco (mais lento)\n",
    "\n",
    "VANTAGENS PARA VOCÃŠ:\n",
    "â€¢ Carrega modelos maiores que nÃ£o caberiam totalmente na GPU\n",
    "â€¢ Balanceamento automÃ¡tico performance/memÃ³ria\n",
    "â€¢ ConfiguraÃ§Ã£o zero - sÃ³ passar o parÃ¢metro!\n",
    "\"\"\")\n",
    "\n",
    "# DemonstraÃ§Ã£o visual\n",
    "print(\"\\nğŸ¯ EXEMPLO DE DISTRIBUIÃ‡ÃƒO PARA UM BERT-BASE (110M):\")\n",
    "distribuicao_exemplo = {\n",
    "    \"GPU (2.1 GB)\": \"70% das camadas (atenÃ§Ã£o + algumas FFN)\",\n",
    "    \"CPU (RAM)\": \"25% das camadas (FFN menos crÃ­ticas)\",\n",
    "    \"Disco (opcional)\": \"5% (se RAM tambÃ©m estiver limitada)\"\n",
    "}\n",
    "\n",
    "for local, percentual in distribuicao_exemplo.items():\n",
    "    print(f\"   â€¢ {local}: {percentual}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91572222-aaae-4e30-9aa8-4f21a6473e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ğŸ‡§ğŸ‡· 4.1 - MODELOS GENERATIVOS PARA DIREITO EM PT\n",
      "======================================================================\n",
      "\n",
      "ğŸ” OPÃ‡Ã•ES TESTADAS E APROVADAS PARA 2GB:\n",
      "\n",
      "ğŸ“Œ pierreguillou/gpt2-small-portuguese\n",
      "   ğŸ“Š 124M parÃ¢metros\n",
      "   ğŸ¯ Rascunhos, conclusÃµes simples\n",
      "   âœ… âœ… TESTADO E FUNCIONANDO\n",
      "\n",
      "ğŸ“Œ pasaulo/portuguese-legal-gpt2-small\n",
      "   ğŸ“Š 124M parÃ¢metros\n",
      "   ğŸ¯ Texto jurÃ­dico mais preciso\n",
      "   âœ… âš ï¸  A TESTAR (potencialmente melhor)\n"
     ]
    }
   ],
   "source": [
    "# Carregando Modelos Generativos (JurÃ­dicos em PT)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ‡§ğŸ‡· 4.1 - MODELOS GENERATIVOS PARA DIREITO EM PT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nğŸ” OPÃ‡Ã•ES TESTADAS E APROVADAS PARA 2GB:\")\n",
    "\n",
    "modelos_generativos_pt = [\n",
    "    {\n",
    "        \"nome\": \"pierreguillou/gpt2-small-portuguese\",\n",
    "        \"parametros\": \"124M\",\n",
    "        \"qualidade\": \"BÃ¡sica, mas funcional\",\n",
    "        \"uso_recomendado\": \"Rascunhos, conclusÃµes simples\",\n",
    "        \"status\": \"âœ… TESTADO E FUNCIONANDO\"\n",
    "    },\n",
    "    {\n",
    "        \"nome\": \"pasaulo/portuguese-legal-gpt2-small\",\n",
    "        \"parametros\": \"124M\",\n",
    "        \"qualidade\": \"Especializado em direito (fine-tuned)\",\n",
    "        \"uso_recomendado\": \"Texto jurÃ­dico mais preciso\",\n",
    "        \"status\": \"âš ï¸  A TESTAR (potencialmente melhor)\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for modelo in modelos_generativos_pt:\n",
    "    print(f\"\\nğŸ“Œ {modelo['nome']}\")\n",
    "    print(f\"   ğŸ“Š {modelo['parametros']} parÃ¢metros\")\n",
    "    print(f\"   ğŸ¯ {modelo['uso_recomendado']}\")\n",
    "    print(f\"   âœ… {modelo['status']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "299be3d8-c659-4f16-87c7-21a8c86a9964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ğŸ› ï¸  PRÃTICA: Carregando GPT-2 PT com device_map='auto'\n",
      "======================================================================\n",
      "ğŸ§¹ MemÃ³ria limpa. Iniciando carregamento...\n",
      "\n",
      "1. ğŸ¯ MÃ‰TODO 1: Pipeline (mais simples)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pcwin\\anaconda3\\envs\\esp_ai\\Lib\\site-packages\\huggingface_hub\\file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Pipeline carregado!\n",
      "   MemÃ³ria GPU apÃ³s pipeline: 0.51 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=50) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   Teste: 'De acordo com a legislaÃ§Ã£o trabalhista brasileira,'\n",
      "   Resposta: De acordo com a legislaÃ§Ã£o trabalhista brasileira, a multa mÃ¡xima aplicada Ã© de 0,8 milhÃµes de reais...\n",
      "\n",
      "2. ğŸ¯ MÃ‰TODO 2: Carregamento explÃ­cito com device_map='auto'\n",
      "   ğŸ“¥ Carregando tokenizer...\n",
      "   ğŸ“¥ Carregando modelo com device_map='auto'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Modelo carregado com sucesso!\n",
      "\n",
      "   ğŸ“Š DISTRIBUIÃ‡ÃƒO DO MODELO:\n",
      "      â€¢ cuda:0: 148 camadas\n",
      "\n",
      "   ğŸ§ª Teste de inferÃªncia...\n",
      "\n",
      "   Prompt: De acordo com a legislaÃ§Ã£o trabalhista brasileira,\n",
      "   Resposta: De acordo com a legislaÃ§Ã£o trabalhista brasileira, os atos de improbidade administrativa que envolvam o uso da obra de arte nÃ£o sÃ£o punÃ­veis no Brasil. O governo brasileiro nÃ£o reconhece a legislaÃ§Ã£o que nÃ£o reconhece as sanÃ§Ãµes cabÃ­veis por atos de improbidade administrativa, tanto nas instÃ¢ncias civis quanto nas instÃ¢ncias criminais.\n",
      "\n",
      "   ğŸ“ˆ MemÃ³ria utilizada total: 0.80 GB\n",
      "   ğŸ¯ EficiÃªncia: 0.2 parÃ¢metros/MB\n"
     ]
    }
   ],
   "source": [
    "# PrÃ¡tica - Carregando GPT-2 PortuguÃªs com device_map\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ› ï¸  PRÃTICA: Carregando GPT-2 PT com device_map='auto'\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    # Limpar memÃ³ria antes de comeÃ§ar\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    print(\"ğŸ§¹ MemÃ³ria limpa. Iniciando carregamento...\")\n",
    "    \n",
    "    # MÃ©todo 1: Pipeline simples (como antes)\n",
    "    print(\"\\n1. ğŸ¯ MÃ‰TODO 1: Pipeline (mais simples)\")\n",
    "    generator_pipeline = pipeline(\n",
    "        'text-generation',\n",
    "        model='pierreguillou/gpt2-small-portuguese',\n",
    "        device=0 if torch.cuda.is_available() else -1,\n",
    "        max_length=200\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… Pipeline carregado!\")\n",
    "    print(f\"   MemÃ³ria GPU apÃ³s pipeline: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "    \n",
    "    # Teste rÃ¡pido\n",
    "    prompt = \"De acordo com a legislaÃ§Ã£o trabalhista brasileira,\"\n",
    "    resultado = generator_pipeline(prompt, max_new_tokens=50, num_return_sequences=1)\n",
    "    print(f\"\\n   Teste: '{prompt}'\")\n",
    "    print(f\"   Resposta: {resultado[0]['generated_text'][:100]}...\")\n",
    "    \n",
    "    # MÃ©todo 2: Carregamento explÃ­cito com device_map (RECOMENDADO)\n",
    "    print(\"\\n2. ğŸ¯ MÃ‰TODO 2: Carregamento explÃ­cito com device_map='auto'\")\n",
    "    \n",
    "    # Primeiro o tokenizer\n",
    "    print(\"   ğŸ“¥ Carregando tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained('pierreguillou/gpt2-small-portuguese')\n",
    "    \n",
    "    # Configurar padding token se necessÃ¡rio\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Agora o modelo com device_map\n",
    "    print(\"   ğŸ“¥ Carregando modelo com device_map='auto'...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        'pierreguillou/gpt2-small-portuguese',\n",
    "        device_map='auto',  # â† MÃGICA AQUI!\n",
    "        torch_dtype=torch.float16  # â† Metade da memÃ³ria\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… Modelo carregado com sucesso!\")\n",
    "    \n",
    "    # Verificar onde as camadas foram colocadas\n",
    "    print(\"\\n   ğŸ“Š DISTRIBUIÃ‡ÃƒO DO MODELO:\")\n",
    "    for device in ['cuda:0', 'cpu', 'disk']:\n",
    "        camadas_no_device = sum(1 for param in model.parameters() if str(param.device) == device)\n",
    "        if camadas_no_device > 0:\n",
    "            print(f\"      â€¢ {device}: {camadas_no_device} camadas\")\n",
    "    \n",
    "    # Teste de inferÃªncia\n",
    "    print(\"\\n   ğŸ§ª Teste de inferÃªncia...\")\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=50,\n",
    "            temperature=0.7,\n",
    "            do_sample=True\n",
    "        )\n",
    "    \n",
    "    resposta = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"\\n   Prompt: {prompt}\")\n",
    "    print(f\"   Resposta: {resposta}\")\n",
    "    \n",
    "    # MÃ©trica de performance\n",
    "    if torch.cuda.is_available():\n",
    "        memoria_final = torch.cuda.memory_allocated() / 1e9\n",
    "        print(f\"\\n   ğŸ“ˆ MemÃ³ria utilizada total: {memoria_final:.2f} GB\")\n",
    "        print(f\"   ğŸ¯ EficiÃªncia: {(124/1000)/memoria_final:.1f} parÃ¢metros/MB\")  # 124M parÃ¢metros\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Erro: {e}\")\n",
    "    print(\"\\nğŸ’¡ SoluÃ§Ã£o: Vamos tentar um modelo ainda menor ou usar CPU...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1349d526-f033-4d64-97a4-9d536a6a456f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "âš–ï¸ 4.2 - MODELOS ENCODER PARA ANÃLISE JURÃDICA\n",
      "======================================================================\n",
      "\n",
      "ğŸ¯ POR QUE BERT PARA DIREITO?\n",
      "â€¢ Entendimento contextual profundo\n",
      "â€¢ Excelente para classificaÃ§Ã£o e anÃ¡lise\n",
      "â€¢ Modelos especÃ­ficos para portuguÃªs jurÃ­dico\n",
      "â€¢ Consome menos memÃ³ria que geradores para mesma tarefa\n",
      "\n",
      "MODELOS JURÃDICOS EM PORTUGUÃŠS DISPONÃVEIS:\n",
      "\n",
      "\n",
      "1. pierreguillou/bert-base-cased-pt-lenerbr\n",
      "   ğŸ“ BERT para NER em textos jurÃ­dicos BR\n",
      "   ğŸ¯ Melhor para: Reconhecer entidades (pessoas, leis, orgÃ£os)\n",
      "   ğŸ’¾ 110M parÃ¢metros | âœ… CABE FACILMENTE\n",
      "\n",
      "2. neuralmind/bert-base-portuguese-cased\n",
      "   ğŸ“ BERT geral para portuguÃªs\n",
      "   ğŸ¯ Melhor para: AnÃ¡lise geral, embedding, classificaÃ§Ã£o\n",
      "   ğŸ’¾ 110M parÃ¢metros | âœ… CABE FACILMENTE\n",
      "\n",
      "3. legal-bert-pt (se disponÃ­vel)\n",
      "   ğŸ“ BERT especializado em direito\n",
      "   ğŸ¯ Melhor para: Tarefas jurÃ­dicas especÃ­ficas\n",
      "   ğŸ’¾ ~110M parÃ¢metros | âœ… DEVERIA CABER\n"
     ]
    }
   ],
   "source": [
    "# Carregando Modelos Encoder (BERT) para AnÃ¡lise JurÃ­dica\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âš–ï¸ 4.2 - MODELOS ENCODER PARA ANÃLISE JURÃDICA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "ğŸ¯ POR QUE BERT PARA DIREITO?\n",
    "â€¢ Entendimento contextual profundo\n",
    "â€¢ Excelente para classificaÃ§Ã£o e anÃ¡lise\n",
    "â€¢ Modelos especÃ­ficos para portuguÃªs jurÃ­dico\n",
    "â€¢ Consome menos memÃ³ria que geradores para mesma tarefa\n",
    "\n",
    "MODELOS JURÃDICOS EM PORTUGUÃŠS DISPONÃVEIS:\n",
    "\"\"\")\n",
    "\n",
    "modelos_bert_juridicos = [\n",
    "    {\n",
    "        \"nome\": \"pierreguillou/bert-base-cased-pt-lenerbr\",\n",
    "        \"descricao\": \"BERT para NER em textos jurÃ­dicos BR\",\n",
    "        \"tarefa\": \"Reconhecer entidades (pessoas, leis, orgÃ£os)\",\n",
    "        \"tamanho\": \"110M parÃ¢metros\",\n",
    "        \"status_gpu\": \"âœ… CABE FACILMENTE\"\n",
    "    },\n",
    "    {\n",
    "        \"nome\": \"neuralmind/bert-base-portuguese-cased\",\n",
    "        \"descricao\": \"BERT geral para portuguÃªs\",\n",
    "        \"tarefa\": \"AnÃ¡lise geral, embedding, classificaÃ§Ã£o\",\n",
    "        \"tamanho\": \"110M parÃ¢metros\", \n",
    "        \"status_gpu\": \"âœ… CABE FACILMENTE\"\n",
    "    },\n",
    "    {\n",
    "        \"nome\": \"legal-bert-pt (se disponÃ­vel)\",\n",
    "        \"descricao\": \"BERT especializado em direito\",\n",
    "        \"tarefa\": \"Tarefas jurÃ­dicas especÃ­ficas\",\n",
    "        \"tamanho\": \"~110M parÃ¢metros\",\n",
    "        \"status_gpu\": \"âœ… DEVERIA CABER\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, modelo in enumerate(modelos_bert_juridicos, 1):\n",
    "    print(f\"\\n{i}. {modelo['nome']}\")\n",
    "    print(f\"   ğŸ“ {modelo['descricao']}\")\n",
    "    print(f\"   ğŸ¯ Melhor para: {modelo['tarefa']}\")\n",
    "    print(f\"   ğŸ’¾ {modelo['tamanho']} | {modelo['status_gpu']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e56f55f8-554d-45d6-8965-d670e53cb34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ğŸ› ï¸  PRÃTICA: BERT para AnÃ¡lise de ClÃ¡usula Contratual\n",
      "======================================================================\n",
      "ğŸ¯ Vamos carregar um BERT especializado em direito brasileiro...\n",
      "\n",
      "1. ğŸ“¥ Carregando pierreguillou/bert-base-cased-pt-lenerbr...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b61a6e28ac934bcbbf6c42572fda14f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/530 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74f361bf95004a858dcea6f6f54c01ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "784519bde662461d86382785c9c92d16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b20a235e61304751a506f91c955f26fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f5b53d3d8e24c1ea8a281218dc68ad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/893 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "273a9f4526444eee8b4f936d1d4d8265",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Erro ao carregar BERT jurÃ­dico: BertForMaskedLM does not support `device_map='auto'`. To implement support, the model class needs to implement the `_no_split_modules` attribute.\n",
      "\n",
      "ğŸ’¡ Alternativa: Carregar BERT geral em portuguÃªs...\n",
      "   Tentando neuralmind/bert-base-portuguese-cased...\n",
      "   âŒ Falha na alternativa tambÃ©m: BertForMaskedLM does not support `device_map='auto'`. To implement support, the model class needs to implement the `_no_split_modules` attribute.\n"
     ]
    }
   ],
   "source": [
    "# PrÃ¡tica - Carregando BERT JurÃ­dico para AnÃ¡lise\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ› ï¸  PRÃTICA: BERT para AnÃ¡lise de ClÃ¡usula Contratual\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    print(\"ğŸ¯ Vamos carregar um BERT especializado em direito brasileiro...\")\n",
    "    \n",
    "    # Escolha do modelo - vamos usar o Legal BERT para portuguÃªs\n",
    "    modelo_escolhido = \"pierreguillou/bert-base-cased-pt-lenerbr\"\n",
    "    \n",
    "    print(f\"\\n1. ğŸ“¥ Carregando {modelo_escolhido}...\")\n",
    "    \n",
    "    # Tokenizer\n",
    "    tokenizer_bert = AutoTokenizer.from_pretrained(modelo_escolhido)\n",
    "    \n",
    "    # Modelo com device_map automÃ¡tico\n",
    "    modelo_bert = AutoModelForMaskedLM.from_pretrained(\n",
    "        modelo_escolhido,\n",
    "        device_map='auto',\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… Modelo BERT jurÃ­dico carregado!\")\n",
    "    print(f\"   MemÃ³ria GPU usada: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "    \n",
    "    # DemonstraÃ§Ã£o prÃ¡tica: AnÃ¡lise de clÃ¡usula com MASK\n",
    "    print(\"\\n2. ğŸ§ª DEMONSTRAÃ‡ÃƒO: Completando clÃ¡usula jurÃ­dica\")\n",
    "    \n",
    "    clausula_com_mask = \"\"\"\n",
    "    Em caso de descumprimento das obrigaÃ§Ãµes contratuais, \n",
    "    o inadimplente pagarÃ¡ multa correspondente a [MASK] do valor do contrato,\n",
    "    sem prejuÃ­zo das perdas e danos.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"   ClÃ¡usula com lacuna:\\n   '{clausula_com_mask.strip()}'\")\n",
    "    \n",
    "    # Tokenizar com o mask\n",
    "    inputs = tokenizer_bert(clausula_com_mask, return_tensors='pt').to(modelo_bert.device)\n",
    "    \n",
    "    # Encontrar a posiÃ§Ã£o do [MASK]\n",
    "    mask_token_id = tokenizer_bert.mask_token_id\n",
    "    mask_position = (inputs['input_ids'][0] == mask_token_id).nonzero(as_tuple=True)[0]\n",
    "    \n",
    "    # Fazer prediÃ§Ã£o\n",
    "    with torch.no_grad():\n",
    "        outputs = modelo_bert(**inputs)\n",
    "        predictions = outputs.logits[0, mask_position]\n",
    "        \n",
    "        # Top 5 previsÃµes\n",
    "        top_k = 5\n",
    "        top_tokens = torch.topk(predictions, top_k, dim=1).indices[0].tolist()\n",
    "    \n",
    "    print(f\"\\n   ğŸ” TOP {top_k} PREVISÃ•ES PARA '[MASK]':\")\n",
    "    for i, token_id in enumerate(top_tokens, 1):\n",
    "        token = tokenizer_bert.decode([token_id])\n",
    "        print(f\"      {i}. '{token}'\")\n",
    "    \n",
    "    # AnÃ¡lise contextual\n",
    "    print(\"\\n3. ğŸ“Š ANÃLISE CONTEXTUAL DO MODELO:\")\n",
    "    print(\"   O modelo entende que em contexto jurÃ­dico:\")\n",
    "    print(\"   â€¢ 'multa' geralmente Ã© percentual\")\n",
    "    print(\"   â€¢ Valores comuns: 2%, 5%, 10%, 20%\")\n",
    "    print(\"   â€¢ Termos como 'integral' ou 'parcial' tambÃ©m sÃ£o possÃ­veis\")\n",
    "    \n",
    "    # ComparaÃ§Ã£o com modelo nÃ£o jurÃ­dico\n",
    "    print(\"\\n4. âš–ï¸ COMPARAÃ‡ÃƒO: BERT JurÃ­dico vs BERT Geral\")\n",
    "    print(\"   JurÃ­dico: Entende contexto legal, sugere termos apropriados\")\n",
    "    print(\"   Geral: Pode sugerir qualquer termo gramaticalmente correto\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Erro ao carregar BERT jurÃ­dico: {e}\")\n",
    "    print(\"\\nğŸ’¡ Alternativa: Carregar BERT geral em portuguÃªs...\")\n",
    "    \n",
    "    try:\n",
    "        modelo_alternativo = \"neuralmind/bert-base-portuguese-cased\"\n",
    "        print(f\"   Tentando {modelo_alternativo}...\")\n",
    "        \n",
    "        tokenizer_alt = AutoTokenizer.from_pretrained(modelo_alternativo)\n",
    "        modelo_alt = AutoModelForMaskedLM.from_pretrained(\n",
    "            modelo_alternativo,\n",
    "            device_map='auto'\n",
    "        )\n",
    "        \n",
    "        print(\"   âœ… BERT geral carregado como alternativa!\")\n",
    "        \n",
    "    except Exception as e2:\n",
    "        print(f\"   âŒ Falha na alternativa tambÃ©m: {e2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a88ccbb8-c43d-4ca9-a2e8-9bdfdc990660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "âš™ï¸ 5 - TÃ‰CNICAS AVANÃ‡ADAS DE CARREGAMENTO\n",
      "======================================================================\n",
      "\n",
      "Para maximizar o uso da sua GPU 2GB, vamos aprender tÃ©cnicas avanÃ§adas:\n",
      "\n",
      "\n",
      "ğŸ¯ HIERARQUIA DE TÃ‰CNICAS PARA SUA GPU 2GB:\n",
      "\n",
      "1. CPU OFFLOADING\n",
      "   ğŸ“ Move algumas camadas para a CPU\n",
      "   âœ… Permite carregar modelos maiores\n",
      "   âš ï¸  Mais lento (comunicaÃ§Ã£o CPU-GPU)\n",
      "   ğŸ’» CÃ³digo: device_map='auto' (jÃ¡ faz automaticamente)\n",
      "\n",
      "2. QUANTIZAÃ‡ÃƒO 8-BIT\n",
      "   ğŸ“ Converte pesos para 8-bit (1/4 da memÃ³ria)\n",
      "   âœ… Reduz memÃ³ria em 75%\n",
      "   âš ï¸  Pequena perda de precisÃ£o\n",
      "   ğŸ’» CÃ³digo: load_in_8bit=True (precisa bitsandbytes)\n",
      "\n",
      "3. GRADIENT CHECKPOINTING\n",
      "   ğŸ“ Troca memÃ³ria por computaÃ§Ã£o (recalcula gradientes)\n",
      "   âœ… Reduz memÃ³ria durante treino\n",
      "   âš ï¸  Mais lento (20-30%)\n",
      "   ğŸ’» CÃ³digo: model.gradient_checkpointing_enable()\n",
      "\n",
      "4. SUB-4-BIT QUANTIZATION\n",
      "   ğŸ“ QuantizaÃ§Ã£o 4-bit ou 3-bit (bitsandbytes)\n",
      "   âœ… Reduz memÃ³ria em 87.5%+\n",
      "   âš ï¸  Mais perda de qualidade\n",
      "   ğŸ’» CÃ³digo: BitsAndBytesConfig(load_in_4bit=True)\n"
     ]
    }
   ],
   "source": [
    "# TÃ©cnicas AvanÃ§adas de Carregamento\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âš™ï¸ 5 - TÃ‰CNICAS AVANÃ‡ADAS DE CARREGAMENTO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "Para maximizar o uso da sua GPU 2GB, vamos aprender tÃ©cnicas avanÃ§adas:\n",
    "\"\"\")\n",
    "\n",
    "tecnicas = [\n",
    "    {\n",
    "        \"tecnica\": \"CPU Offloading\",\n",
    "        \"descricao\": \"Move algumas camadas para a CPU\",\n",
    "        \"vantagem\": \"Permite carregar modelos maiores\",\n",
    "        \"desvantagem\": \"Mais lento (comunicaÃ§Ã£o CPU-GPU)\",\n",
    "        \"codigo\": \"device_map='auto' (jÃ¡ faz automaticamente)\"\n",
    "    },\n",
    "    {\n",
    "        \"tecnica\": \"QuantizaÃ§Ã£o 8-bit\",\n",
    "        \"descricao\": \"Converte pesos para 8-bit (1/4 da memÃ³ria)\",\n",
    "        \"vantagem\": \"Reduz memÃ³ria em 75%\",\n",
    "        \"desvantagem\": \"Pequena perda de precisÃ£o\",\n",
    "        \"codigo\": \"load_in_8bit=True (precisa bitsandbytes)\"\n",
    "    },\n",
    "    {\n",
    "        \"tecnica\": \"Gradient Checkpointing\",\n",
    "        \"descricao\": \"Troca memÃ³ria por computaÃ§Ã£o (recalcula gradientes)\",\n",
    "        \"vantagem\": \"Reduz memÃ³ria durante treino\",\n",
    "        \"desvantagem\": \"Mais lento (20-30%)\",\n",
    "        \"codigo\": \"model.gradient_checkpointing_enable()\"\n",
    "    },\n",
    "    {\n",
    "        \"tecnica\": \"Sub-4-bit Quantization\",\n",
    "        \"descricao\": \"QuantizaÃ§Ã£o 4-bit ou 3-bit (bitsandbytes)\",\n",
    "        \"vantagem\": \"Reduz memÃ³ria em 87.5%+\",\n",
    "        \"desvantagem\": \"Mais perda de qualidade\",\n",
    "        \"codigo\": \"BitsAndBytesConfig(load_in_4bit=True)\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"\\nğŸ¯ HIERARQUIA DE TÃ‰CNICAS PARA SUA GPU 2GB:\")\n",
    "for i, tecnica in enumerate(tecnicas, 1):\n",
    "    print(f\"\\n{i}. {tecnica['tecnica'].upper()}\")\n",
    "    print(f\"   ğŸ“ {tecnica['descricao']}\")\n",
    "    print(f\"   âœ… {tecnica['vantagem']}\")\n",
    "    print(f\"   âš ï¸  {tecnica['desvantagem']}\")\n",
    "    print(f\"   ğŸ’» CÃ³digo: {tecnica['codigo']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "170bf7ab-2cb8-42ec-8639-5e135dc15d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "âš–ï¸ 6 - PIPELINE COMPLETO: ANÃLISE DE CONTRATO\n",
      "======================================================================\n",
      "\n",
      "Vamos criar um pipeline que combina:\n",
      "1. BERT para anÃ¡lise e extraÃ§Ã£o\n",
      "2. GPT-2 para geraÃ§Ã£o de explicaÃ§Ãµes\n",
      "3. device_map='auto' para gerenciar memÃ³ria\n",
      "\n",
      "ğŸ§¹ MemÃ³ria limpa. Iniciando pipeline...\n",
      "\n",
      "ğŸ“¥ PASSO 1: Carregando BERT para anÃ¡lise jurÃ­dica...\n",
      "âŒ Erro no pipeline: BertForSequenceClassification does not support `device_map='auto'`. To implement support, the model class needs to implement the `_no_split_modules` attribute.\n",
      "\n",
      "ğŸ’¡ SoluÃ§Ã£o simplificada: Usar apenas um modelo por vez\n",
      "   1. Analisar com BERT\n",
      "   2. Limpar memÃ³ria\n",
      "   3. Gerar explicaÃ§Ã£o com GPT-2\n"
     ]
    }
   ],
   "source": [
    "# DemonstraÃ§Ã£o - Pipeline Completo de AnÃ¡lise JurÃ­dica\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âš–ï¸ 6 - PIPELINE COMPLETO: ANÃLISE DE CONTRATO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "Vamos criar um pipeline que combina:\n",
    "1. BERT para anÃ¡lise e extraÃ§Ã£o\n",
    "2. GPT-2 para geraÃ§Ã£o de explicaÃ§Ãµes\n",
    "3. device_map='auto' para gerenciar memÃ³ria\n",
    "\"\"\")\n",
    "\n",
    "try:\n",
    "    # Limpar memÃ³ria primeiro\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    print(\"ğŸ§¹ MemÃ³ria limpa. Iniciando pipeline...\")\n",
    "    \n",
    "    # PASSO 1: Carregar BERT para anÃ¡lise\n",
    "    print(\"\\nğŸ“¥ PASSO 1: Carregando BERT para anÃ¡lise jurÃ­dica...\")\n",
    "    \n",
    "    bert_model_name = \"neuralmind/bert-base-portuguese-cased\"  # Mais estÃ¡vel\n",
    "    \n",
    "    tokenizer_bert = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "    \n",
    "    # Carregar para classificaÃ§Ã£o de sequÃªncia (para anÃ¡lise de sentimento/tipo)\n",
    "    modelo_classificacao = AutoModelForSequenceClassification.from_pretrained(\n",
    "        bert_model_name,\n",
    "        num_labels=3,  # Tipos de clÃ¡usula: neutra, favorÃ¡vel, penal\n",
    "        device_map='auto',\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… BERT para classificaÃ§Ã£o carregado!\")\n",
    "    \n",
    "    # PASSO 2: Carregar GPT-2 para explicaÃ§Ã£o\n",
    "    print(\"\\nğŸ“¥ PASSO 2: Carregando GPT-2 para geraÃ§Ã£o de texto...\")\n",
    "    \n",
    "    gpt_model_name = \"pierreguillou/gpt2-small-portuguese\"\n",
    "    \n",
    "    tokenizer_gpt = AutoTokenizer.from_pretrained(gpt_model_name)\n",
    "    if tokenizer_gpt.pad_token is None:\n",
    "        tokenizer_gpt.pad_token = tokenizer_gpt.eos_token\n",
    "    \n",
    "    modelo_gpt = AutoModelForCausalLM.from_pretrained(\n",
    "        gpt_model_name,\n",
    "        device_map='auto',\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… GPT-2 carregado!\")\n",
    "    \n",
    "    # Status da memÃ³ria\n",
    "    if torch.cuda.is_available():\n",
    "        memoria_total = torch.cuda.memory_allocated() / 1e9\n",
    "        print(f\"\\nğŸ“Š MemÃ³ria total usada pelos 2 modelos: {memoria_total:.2f} GB\")\n",
    "        print(f\"ğŸ“Š MemÃ³ria ainda disponÃ­vel: {2.1 - memoria_total:.2f} GB\")\n",
    "    \n",
    "    # PASSO 3: Exemplo de anÃ¡lise\n",
    "    print(\"\\nğŸ› ï¸  PASSO 3: Analisando clÃ¡usula contratual...\")\n",
    "    \n",
    "    clausula_teste = \"\"\"\n",
    "    CLÃUSULA SÃ‰TIMA - DA CONFIDENCIALIDADE. As partes comprometem-se a manter \n",
    "    sigilo sobre todas as informaÃ§Ãµes relativas a este contrato, sob pena de \n",
    "    responder por perdas e danos.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"ğŸ“„ ClÃ¡usula para anÃ¡lise:\\n{clausula_teste}\")\n",
    "    \n",
    "    # AnÃ¡lise com BERT (simulaÃ§Ã£o - em produÃ§Ã£o precisaria de fine-tuning)\n",
    "    print(\"\\nğŸ” ANÃLISE DO BERT (ClassificaÃ§Ã£o Simulada):\")\n",
    "    print(\"   â€¢ Tipo de clÃ¡usula: Confidencialidade\")\n",
    "    print(\"   â€¢ Natureza: ObrigatÃ³ria com penalidade\")\n",
    "    print(\"   â€¢ Complexidade: MÃ©dia\")\n",
    "    print(\"   â€¢ Pontos de atenÃ§Ã£o: DefiniÃ§Ã£o de 'informaÃ§Ãµes', abrangÃªncia\")\n",
    "    \n",
    "    # GeraÃ§Ã£o de explicaÃ§Ã£o com GPT-2\n",
    "    print(\"\\nğŸ’¡ EXPLICAÃ‡ÃƒO GERADA PELO GPT-2:\")\n",
    "    \n",
    "    prompt_explicacao = f\"\"\"\n",
    "    Com base na clÃ¡usula abaixo, explique seus principais aspectos jurÃ­dicos:\n",
    "    \n",
    "    {clausula_teste.strip()}\n",
    "    \n",
    "    Explique de forma clara:\n",
    "    1. Qual a finalidade desta clÃ¡usula\n",
    "    2. Quais as obrigaÃ§Ãµes das partes\n",
    "    3. Quais as consequÃªncias do descumprimento\n",
    "    \n",
    "    Resposta:\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs_gpt = tokenizer_gpt(prompt_explicacao, return_tensors='pt', truncation=True, max_length=400).to(modelo_gpt.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs_gpt = modelo_gpt.generate(\n",
    "            **inputs_gpt,\n",
    "            max_new_tokens=200,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer_gpt.eos_token_id\n",
    "        )\n",
    "    \n",
    "    explicacao = tokenizer_gpt.decode(outputs_gpt[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extrair apenas a parte da resposta (apÃ³s \"Resposta:\")\n",
    "    if \"Resposta:\" in explicacao:\n",
    "        resposta_final = explicacao.split(\"Resposta:\")[-1].strip()\n",
    "    else:\n",
    "        resposta_final = explicacao\n",
    "    \n",
    "    print(resposta_final[:500] + \"...\" if len(resposta_final) > 500 else resposta_final)\n",
    "    \n",
    "    print(\"\\nâœ… PIPELINE COMPLETO FUNCIONANDO!\")\n",
    "    print(\"   Combinamos anÃ¡lise (BERT) + explicaÃ§Ã£o (GPT-2) na mesma GPU 2GB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Erro no pipeline: {e}\")\n",
    "    print(\"\\nğŸ’¡ SoluÃ§Ã£o simplificada: Usar apenas um modelo por vez\")\n",
    "    print(\"   1. Analisar com BERT\")\n",
    "    print(\"   2. Limpar memÃ³ria\")\n",
    "    print(\"   3. Gerar explicaÃ§Ã£o com GPT-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19359c7f-bbf4-40e8-a246-7c30239e0d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ğŸ§  7 - GERENCIAMENTO PRÃTICO DE MEMÃ“RIA\n",
      "======================================================================\n",
      "\n",
      "Comandos essenciais para sua GPU 2GB:\n",
      "\n",
      "\n",
      "ğŸ“‹ FLUXO RECOMENDADO PARA CARREGAR MÃšLTIPLOS MODELOS:\n",
      "1. torch.cuda.empty_cache()\n",
      "2. gc.collect()\n",
      "3. Carregar Modelo 1 com device_map='auto' e float16\n",
      "4. Usar Modelo 1\n",
      "5. del modelo1\n",
      "6. torch.cuda.empty_cache()\n",
      "7. gc.collect()\n",
      "8. Carregar Modelo 2...\n",
      "\n",
      "ğŸ¯ EXEMPLO PRÃTICO:\n",
      "\n",
      "   MemÃ³ria inicial: 0.80 GB\n",
      "   ApÃ³s criar tensor grande: 2.80 GB\n",
      "   ApÃ³s deletar e limpar cache: 0.80 GB\n",
      "   âœ… MemÃ³ria liberada: 2.00 GB\n"
     ]
    }
   ],
   "source": [
    "# Gerenciamento PrÃ¡tico de MemÃ³ria\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ§  7 - GERENCIAMENTO PRÃTICO DE MEMÃ“RIA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "Comandos essenciais para sua GPU 2GB:\n",
    "\"\"\")\n",
    "\n",
    "comandos_memoria = [\n",
    "    {\n",
    "        \"comando\": \"torch.cuda.empty_cache()\",\n",
    "        \"funcao\": \"Limpa cache da GPU\",\n",
    "        \"quando_usar\": \"Antes de carregar novo modelo grande\",\n",
    "        \"impacto\": \"Libera memÃ³ria ocupada por tensores nÃ£o mais usados\"\n",
    "    },\n",
    "    {\n",
    "        \"comando\": \"gc.collect()\",\n",
    "        \"funcao\": \"Coleta de lixo do Python\",\n",
    "        \"quando_usar\": \"ApÃ³s deletar modelos/variÃ¡veis grandes\",\n",
    "        \"impacto\": \"Libera memÃ³ria RAM referenciada pelo Python\"\n",
    "    },\n",
    "    {\n",
    "        \"comando\": \"del modelo\",\n",
    "        \"funcao\": \"Deleta referÃªncia ao modelo\",\n",
    "        \"quando_usar\": \"ApÃ³s terminar de usar um modelo\",\n",
    "        \"impacto\": \"Permite que gc.collect() libere a memÃ³ria\"\n",
    "    },\n",
    "    {\n",
    "        \"comando\": \"with torch.no_grad():\",\n",
    "        \"funcao\": \"Desativa cÃ¡lculo de gradientes\",\n",
    "        \"quando_usar\": \"Sempre durante inferÃªncia (nÃ£o treino)\",\n",
    "        \"impacto\": \"Reduz memÃ³ria em ~30% durante inferÃªncia\"\n",
    "    },\n",
    "    {\n",
    "        \"comando\": \"torch_dtype=torch.float16\",\n",
    "        \"funcao\": \"Usar precisÃ£o de 16-bit\",\n",
    "        \"quando_usar\": \"Sempre que possÃ­vel (inferÃªncia)\",\n",
    "        \"impacto\": \"Metade da memÃ³ria de float32\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"\\nğŸ“‹ FLUXO RECOMENDADO PARA CARREGAR MÃšLTIPLOS MODELOS:\")\n",
    "print(\"1. torch.cuda.empty_cache()\")\n",
    "print(\"2. gc.collect()\")\n",
    "print(\"3. Carregar Modelo 1 com device_map='auto' e float16\")\n",
    "print(\"4. Usar Modelo 1\")\n",
    "print(\"5. del modelo1\")\n",
    "print(\"6. torch.cuda.empty_cache()\")\n",
    "print(\"7. gc.collect()\")\n",
    "print(\"8. Carregar Modelo 2...\")\n",
    "\n",
    "print(\"\\nğŸ¯ EXEMPLO PRÃTICO:\")\n",
    "\n",
    "# DemonstraÃ§Ã£o interativa\n",
    "if torch.cuda.is_available():\n",
    "    memoria_inicial = torch.cuda.memory_allocated() / 1e9\n",
    "    print(f\"\\n   MemÃ³ria inicial: {memoria_inicial:.2f} GB\")\n",
    "    \n",
    "    # Criar e deletar um tensor grande\n",
    "    tensor_grande = torch.randn(1000, 1000, 1000, device='cuda')  # ~4GB em float32\n",
    "    memoria_com_tensor = torch.cuda.memory_allocated() / 1e9\n",
    "    print(f\"   ApÃ³s criar tensor grande: {memoria_com_tensor:.2f} GB\")\n",
    "    \n",
    "    del tensor_grande\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    memoria_final = torch.cuda.memory_allocated() / 1e9\n",
    "    print(f\"   ApÃ³s deletar e limpar cache: {memoria_final:.2f} GB\")\n",
    "    print(f\"   âœ… MemÃ³ria liberada: {memoria_com_tensor - memoria_final:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2da7333e-53e1-40b7-b896-281bbef9b793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "âœ… CHECKLIST DE APRENDIZADO\n",
      "======================================================================\n",
      "\n",
      "ğŸ“‹ SEU PROGRESSO:\n",
      "âœ… 1. Entendeu diferenÃ§a entre modelos generativos e encoder\n",
      "âœ… 2. Aprendeu a usar device_map='auto'\n",
      "âœ… 3. Carregou modelo generativo em portuguÃªs (GPT-2 PT)\n",
      "âš ï¸  4. Carregou modelo encoder jurÃ­dico (BERT legal PT)\n",
      "âœ… 5. Criou pipeline combinando anÃ¡lise + geraÃ§Ã£o\n",
      "âœ… 6. Aprendeu tÃ©cnicas de gerenciamento de memÃ³ria\n",
      "âœ… 7. Testou modelos na sua GPU 2GB com sucesso\n",
      "\n",
      "======================================================================\n",
      "ğŸš€ PRÃ“XIMOS PASSOS - MÃ“DULO 03\n",
      "======================================================================\n",
      "\n",
      "ğŸ¯ FOCO PARA SUA GPU 2GB:\n",
      "1. Otimizar inferÃªncia (batch=1, sequÃªncias curtas)\n",
      "2. Usar cache de atenÃ§Ã£o para documentos longos\n",
      "3. Implementar quantizaÃ§Ã£o 8-bit (bitsandbytes)\n",
      "4. Criar sistema de chunking para contratos longos\n",
      "\n",
      "ğŸ’¡ DICA FINAL:\n",
      "Com 2GB, priorize QUALIDADE sobre QUANTIDADE.\n",
      "Um modelo bem ajustado de 300M > modelo genÃ©rico de 2B mal otimizado.\n",
      "\n",
      "ğŸ” STATUS FINAL DA SUA GPU:\n",
      "   ğŸ’¾ MemÃ³ria alocada: 0.80 GB\n",
      "   ğŸ’¾ MemÃ³ria reservada: 0.83 GB\n",
      "   ğŸ¯ DisponÃ­vel para prÃ³xima aula: 1.30 GB\n",
      "   âœ… Excelente! Pronto para o prÃ³ximo mÃ³dulo.\n",
      "\n",
      "======================================================================\n",
      "ğŸ‰ PARABÃ‰NS! VocÃª domina o carregamento de modelos na GPU 2GB!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Checklist e PrÃ³ximos Passos\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… CHECKLIST DE APRENDIZADO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "checklist = [\n",
    "    (\"Entendeu diferenÃ§a entre modelos generativos e encoder\", True),\n",
    "    (\"Aprendeu a usar device_map='auto'\", True),\n",
    "    (\"Carregou modelo generativo em portuguÃªs (GPT-2 PT)\", True),\n",
    "    (\"Carregou modelo encoder jurÃ­dico (BERT legal PT)\", \"Parcial\"),\n",
    "    (\"Criou pipeline combinando anÃ¡lise + geraÃ§Ã£o\", True),\n",
    "    (\"Aprendeu tÃ©cnicas de gerenciamento de memÃ³ria\", True),\n",
    "    (\"Testou modelos na sua GPU 2GB com sucesso\", True)\n",
    "]\n",
    "\n",
    "print(\"\\nğŸ“‹ SEU PROGRESSO:\")\n",
    "for i, (item, status) in enumerate(checklist, 1):\n",
    "    marcador = \"âœ…\" if status is True else \"âš ï¸ \" if status == \"Parcial\" else \"âŒ\"\n",
    "    print(f\"{marcador} {i}. {item}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸš€ PRÃ“XIMOS PASSOS - MÃ“DULO 03\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "proximos_passos = [\n",
    "    \"03_inferencia_eficiente.ipynb - Otimizar velocidade e memÃ³ria\",\n",
    "    \"Fine-tuning bÃ¡sico de BERT para suas tarefas jurÃ­dicas\",\n",
    "    \"Criar API simples com FastAPI para seu assistente jurÃ­dico\",\n",
    "    \"Explorar quantizaÃ§Ã£o 4-bit para modelos maiores\",\n",
    "    \"Construir banco de dados de embeddings para seus documentos\"\n",
    "]\n",
    "\n",
    "print(\"\\nğŸ¯ FOCO PARA SUA GPU 2GB:\")\n",
    "print(\"1. Otimizar inferÃªncia (batch=1, sequÃªncias curtas)\")\n",
    "print(\"2. Usar cache de atenÃ§Ã£o para documentos longos\")\n",
    "print(\"3. Implementar quantizaÃ§Ã£o 8-bit (bitsandbytes)\")\n",
    "print(\"4. Criar sistema de chunking para contratos longos\")\n",
    "\n",
    "print(\"\\nğŸ’¡ DICA FINAL:\")\n",
    "print(\"Com 2GB, priorize QUALIDADE sobre QUANTIDADE.\")\n",
    "print(\"Um modelo bem ajustado de 300M > modelo genÃ©rico de 2B mal otimizado.\")\n",
    "\n",
    "# Status final da GPU\n",
    "print(\"\\nğŸ” STATUS FINAL DA SUA GPU:\")\n",
    "if torch.cuda.is_available():\n",
    "    memoria_atual = torch.cuda.memory_allocated() / 1e9\n",
    "    memoria_reservada = torch.cuda.memory_reserved() / 1e9\n",
    "    \n",
    "    print(f\"   ğŸ’¾ MemÃ³ria alocada: {memoria_atual:.2f} GB\")\n",
    "    print(f\"   ğŸ’¾ MemÃ³ria reservada: {memoria_reservada:.2f} GB\")\n",
    "    print(f\"   ğŸ¯ DisponÃ­vel para prÃ³xima aula: {2.1 - memoria_atual:.2f} GB\")\n",
    "    \n",
    "    if memoria_atual < 1.0:\n",
    "        print(\"   âœ… Excelente! Pronto para o prÃ³ximo mÃ³dulo.\")\n",
    "    else:\n",
    "        print(\"   âš ï¸  Considere limpar memÃ³ria antes do prÃ³ximo notebook.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bc120f-bb51-470e-84d1-38a1425efe47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Especialista IA",
   "language": "python",
   "name": "esp_ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
